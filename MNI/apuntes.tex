\part{Tema 1. Introducción a los problemas del Análisis Numérico}

\section{Introducción a los métodos numéricos: algoritmo}
Comenzaremos con un ejemplo de recurrencia en el que observaremos que al redondear el primer valor, se acumula el error y los siguientes valores se desbordan.\\
Sea n $\geq$ 1 y la recurrencia $x_{n}$:=$\int_{0}^{1} x^{n}e^{x}dx$.\\
Si resolvemos la integral para n = 0, tenemos que $x_{0}$ = e - 1\\
Y para n $\in$ $\mathbb{N}$, tenemos que $x_{n}$ = e - n$x_{n-1}$\\
Por lo que esta sucesión, $\lbrace x_{n} \rbrace_{n\geq1}$ $\subset$ $\mathbb{R_{+}}$, es decreciente y tiende a 0, es decir, $ \lim_{n \to \infty} x_{n} = 0 $ \\
Veamos que si redondeamos $x_{0}$ se acumula el error.\\
Si n = 12 tenemos que $x_{12}$ = 0.1951\\
Redondeando $x_{0}$ = 1.7183 e iterando según este valor hasta n = 12, obtenemos que $x_{12}$ = 8704.39\\
Luego, este valor es, con diferencia, mayor que el que habíamos calculado sin redondeo y $x_{n}$ no tiende a 0.\\
Concluimos que el redondeo, a veces, conlleva errores muy grandes.

\subsection{Espacios normados}
Si usamos las normas en los problemas numéricos, sabremos si los problemas están bien planteados, los errores cometidos, la convergencia...

\begin{ndef}[Norma] 
Sea E un espacio vectorial real, diremos que una aplicacion $\Vert$·$\Vert$: E $\rightarrow$ $\mathbb{R}$ es una $\textbf{norma}$ en E si verifica las siguientes propiedades:
	\begin{nlist}
	\item Sea x $\in$ E $\Rightarrow$ $\Vert$·$\Vert$ $\geq$ 0.\\
	Además, $\Vert$x$\Vert$ = 0 $\Leftrightarrow$ x = 0
	\item Sean x,y $\in$ E $\Rightarrow$ $\Vert$x + y$\Vert$ $\leq$ $\Vert$x$\Vert$ + $\Vert$y$\Vert$ $\quad$ (desigualdad triangular)
	\item Sean x $\in$ E, $\lambda$ $\in$ $\mathbb{R}$ $\Rightarrow$ $\Vert$ $	\lambda$x $\Vert$ = $\vert$ $\lambda$ $\vert$ $\Vert$x$\Vert$
	\end{nlist}
\end{ndef}

\begin{ndef}[Espacio normado]
Sea E un espacio vectorial real. Si este espacio admite una norma, entonces E se llama $\textbf{espacio normado}$.
\end{ndef}

Aunque trabajaremos en $\mathbb{R}$, en $\mathbb{C}$ es lo mismo.\\
Veamos la interpretación geométrica de la desigualdad triangular, usando la norma euclídea $\mathbb{R}^2$ o $\mathbb{R}^3$.\\ 
\includegraphics[scale=0.2]{media/desigualdadtriangular.png}

Las siguientes normas son las que vamos a utilizar.

\begin{ndef}[Norma p]
Sean E = $\mathbb{R}^N$, p $\geq$ 1 y x $\in$ $\mathbb{R}^N$, entonces:
\[ \Vert x \Vert _{p} := \left( \sum_{j=1}^{N} \vert x_{j} \vert ^p \right) ^{1/p} \]
Si p = 2, entonces la norma es $\textbf{euclídea}$.
\end{ndef}

\begin{ndef}[Norma del máximo]
Sea E = $\mathbb{R}^N$ y x $\in$ $\mathbb{R}^N$, entonces:
\[ \Vert x \Vert _{\infty} := max \lbrace \vert x_{j} \vert : j = 1,...,N \rbrace \]
\end{ndef}

\begin{ndef}[Norma de Frobenius]
Sea E = $\mathbb{R}^{M \times N}$ y A $\in$ $\mathbb{R}^{M \times N}$, entonces:
\[ \Vert A \Vert _F := \sqrt{\sum_{i=1}^{M} \sum_{j=1}^{N} a_{ij}^2} \]
\end{ndef}

\textit{Aclaración:}\\
\textit{Si estamos en un espacio vectorial real C([a,b]), esto significa que este espacio está compuesto por todas las funciones}\\
\textit{continuas en el intervalo cerrado [a,b].}\\
\textit{Si el espacio es $C^k$([a,b]), significa que está formado por las funciones de clase k, es decir, funciones derivables hasta}\\
\textit{orden k y esas derivadas son continuas.}

\begin{ndef}[Norma del máximo]
Sea E = C([a,b]) y f $\in$ C([a,b]), entonces:
\[ \Vert f \Vert _\infty := max \; \left\lbrace \vert f(x) \vert : a \leq x \leq b \right\rbrace \]
\end{ndef}

\begin{ndef}
Sean E = $C^k$([a,b]), k $\in$ $\mathbb{N}$ y f $\in$ $C^k$([a,b]) entonces:
\[ \Vert f \Vert _k := max \; \left\lbrace \Vert f^{(j)} \Vert : j = 0,...,k \right\rbrace \]
\end{ndef}

Ahora que ya tenemos definidas las normas, podemos calcular el error cometido al aproximar los vectores.

\begin{ndef}[Error absoluto]
Sea E un espacio normado, x $\in$ E y x* $\in$ una aproximación de x, entonces la siguiente operación calcula el error absoluto:
\[ \Vert x^* - x \Vert \]
\end{ndef}

\begin{ndef}[Error relativo]
Sea E un espacio normado, x $\in$ E y x* $\in$ una aproximación de x, entonces la siguiente operación calcula el error relativo:
\[  \frac {\Vert x^* - x \Vert}{ \Vert x \Vert} \]
\end{ndef}

Veamos una aplicación de estos errores.

\begin{ejer}
Calcula los errores absolutos y relativos de:
	\begin{nlist}
	\item E = $\mathbb{R}$, x = 1/4, x* = 0.23
	\item E = $\mathbb{R}^3$, x = (1/5,2,1), x* = (0.19,2.2,0.9)
	\item E = C([0,$\pi$/2]), f(t) = sen(t), f*(t) = t
	\end{nlist}
\end{ejer}

\begin{sol}
	\begin{nlist}
	\item error absoluto: $\vert x^* - x \vert = \vert 0.23 - 1/4 \vert = 0.02 $\\
	error relativo: $\frac{\vert x^* - x \vert}{\vert x \vert} = \frac{\vert 0.23 - 1/4 \vert}{\vert 1/4 \vert} = 0.08$
	\item error absoluto: $\Vert x^* - x \Vert _\infty = \Vert (0.19,2.2,0.9) - (1/5,2,1) \Vert _\infty = \Vert (-0.01,0.2,-0.1) \Vert _\infty =$\\ $= max \lbrace 0.01,0.2,0.1\rbrace = 0.2 $\\
	error relativo: $\frac{\Vert x^* - x \Vert _\infty}{\Vert x \Vert _\infty} = \frac{\Vert (-0.01,0.2,-0.1) \Vert _\infty}{\Vert (1/5,2,1) \Vert _\infty} = \frac{0.2}{2} = 0.1 $\\
	\item error absoluto: $\Vert f^* - f \Vert _\infty = \Vert t - sen(t) \Vert _\infty = \frac{\pi}{2} - 1$\\
	error relativo: $\frac{\Vert f^* - f \Vert _\infty}{\Vert f \Vert _\infty} = \frac{\pi}{2} - 1 $
	\end{nlist}
\end{sol}

\begin{ndef}[Distancia]
Se define la $\textbf{distancia}$ entre dos vectores x,y $\in$ E como
\[ dist(x,y) := \Vert x - y \Vert \]
\end{ndef}

\begin{ndef}
Se dice que $\lbrace x_n \rbrace _{n \geq 1}$ en E $\textbf{converge}$ a $x_0$ $\in$ E sii
\[ \forall \varepsilon > 0 \Rightarrow \left[ \exists n_0 \in \mathbb{N} : n \geq n_0 \Rightarrow \Vert x_n - x_0 \Vert < \varepsilon \right] \]
es decir,
\[ \lim_{n \rightarrow \infty} x_n = x_0  \Leftrightarrow \lim_{n \rightarrow \infty} \Vert x_n - x_0 \Vert = 0 \]
\end{ndef}

\begin{ndef}
Sean X, Y subconjuntos no vacíos de sendos espacios normados y sea f: X $\rightarrow$ Y, diremos que f es $\textbf{continua}$ en $x_{0}$ $\in$ X si
\[ \forall \varepsilon > 0 \Rightarrow \left[ \exists \delta > 0 : x \in X \wedge \Vert x - x_0 \Vert < \delta \Rightarrow \Vert f(x) - f(x_0) \Vert < \varepsilon \right] \]
\end{ndef}

\begin{nprop}
Sea x $\in \mathbb{R}^N \Rightarrow \Vert x \Vert _\infty \leq \Vert x \Vert _1 \leq N\Vert x \Vert _\infty $ 
\end{nprop}

\begin{ndef}
Sean $\Vert$ · $\Vert$ y $\Vert$ · $\Vert _*$ dos normas, se dice que son $\textbf{equivalentes}$ si $\exists c_1, c_2 > 0$ tales que
\[ \forall x \in E \Rightarrow c_1\Vert x \Vert \leq \Vert x \Vert _* \leq c_2\Vert x \Vert \] 
\end{ndef}

\begin{nprop}
Sean $\Vert$ · $\Vert$ y $\Vert$ · $\Vert _*$ dos normas, entonces la convergencia de sucesiones y la continuidad son equivalentes para ambas normas. 
\end{nprop}

\begin{nth}
Todas las normas en un espacio normado finito dimensional son equivalentes.
\end{nth}

Observemos que para calcular el límite de la norma del máximo, tenemos que calcular el límite de cada coordenada.

\begin{nprop}
Sea $\mathbb{R}^N$ un espacio normado finito dimensional y consideremos la norma $\Vert$·$\Vert _\infty$ en este espacio, entonces:
\[ \lim_{n \rightarrow \infty}x_n = x_0 \Leftrightarrow \lim_{n \geq 1}(x_n)_j = (x_0)_j \qquad \forall j\in \lbrace 1,...,N \rbrace \]
\end{nprop}

\begin{proof}
\[ \lim_{n \rightarrow \infty} x_n = x_0 \quad \Leftrightarrow \quad \lim_{n \rightarrow \infty} \Vert x_n - x_0 \Vert _\infty = 0 \quad \Leftrightarrow \quad 0 < max \; \lbrace \vert (x_n - x_0)_j \vert : j = 1,...,N \rbrace = 0 \quad \rightarrow \quad \lim_{n \geq 1}(x_n)_j = (x_0)_j \]
\end{proof}

Un ejemplo de aplicación de esta proposición es el siguiente.

\begin{ejemplo}
$\lim_{n \geq 1} \left( \left( 1 + \frac{1}{n} \right) ^n , \frac{(-1)^n}{n^2} \right) = (e,0)$
\end{ejemplo}

La anterior proposición también se puede aplicar para cualquier norma de $\mathbb{R}^N$ y de $\mathbb{R}^{M \times N}$.

\begin{ejer}
Comprueba que la norma del máximo en C([0,1]) no es equivalente a la norma $\Vert$·$\Vert _1$ definida para cada f $\in$ C([0,1]) como
\[ \Vert f \Vert _1 := \int_0^1 \vert f(x) \vert dx \]
(Indicación: para cada n $\geq$ 2, considera la función $f_n$ cuya gráfica es la poligonal que une los puntos (0,0), (1/n,1), (2/n,0), (1,0)).
\end{ejer}

\begin{sol}
Tenemos que\\
\[ E = C([0,1]) \]
\[ \Vert f \Vert _\infty = max \left\lbrace \vert f(x) \vert : 0 \leq x \leq 1 \right\rbrace \]
\[ \Vert f \Vert _1 := \int_0^1 \vert f(x) \vert dx \]
Luego\\
$\Vert f_n \Vert _\infty = 1$ y $\Vert f_n \Vert _1 = 1/n$ (que coincide con el área).\\
Si fueran equivalentes, entonces
\[ \exists \alpha , \beta > 0 f \in E \Rightarrow \alpha \Vert f_n \Vert _1 \leq \Vert f_n \Vert _\infty \leq \beta \Vert f_n \Vert _1 \]
Lo cual es una contradicción, porque n $\geq$ 1 $\Rightarrow \Vert f_n \Vert _\infty \leq \beta \Vert f_n \Vert _1 \Leftrightarrow 1 \leq \frac{\beta}{n} \Leftrightarrow n \leq \beta$ $\;$ y n no está acotada.\\
Por lo que no son equivalentes.
\end{sol}

\begin{nprop}
Sean M, N $\in$ $\mathbb{N}$ y consideremos sendas normas en $\mathbb{R}^N$ y $\mathbb{R}^M$, que sin lugar a ambigüedad notaremos indeferentemente como $\Vert$·$\Vert$. Entonces la aplicación que notaremos igualmente como $\Vert$·$\Vert$ define una norma en $\mathbb{R}^{M \times N}$:
\[ \Vert A \Vert := sup \; \left\lbrace \Vert Ax \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace \qquad \forall A \in \mathbb{R}^{M \times N} \]
\end{nprop}

\begin{ndef}
Se define la norma \textbf{inducida} en $\mathbb{R}^{M \times N}$ como:
\[ \Vert A \Vert := sup \; \left\lbrace \Vert Ax \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace \qquad \forall A \in \mathbb{R}^{M \times N} \]
\end{ndef}

\begin{nprop}
Con la notación de la proposición anterior, si $A \in \mathbb{R}^{M \times N}$ entonces
\[ \Vert A \Vert := sup \; \left\lbrace \frac{\Vert Ax \Vert}{\Vert x \Vert } : x \in \mathbb{R}^N \wedge x \neq 0 \right\rbrace \]
En particular,
\[ \Vert Ax \Vert \leq \Vert A \Vert \Vert x \Vert \]
\end{nprop}

\begin{nprop}
Consideremos la norma $\Vert$·$\Vert _1$ en $\mathbb{R}^N$ y en $\mathbb{R}^M$, entonces la norma $\Vert$·$\Vert _1$ inducida en $\mathbb{R}^{M \times N}$ es
\[ \Vert A \Vert _1 = max \; \left\lbrace \sum_{i=1}^M \vert a_{ij} \vert : j = 1,...,N \right\rbrace \qquad \forall A \in \mathbb{R}^{M \times N} \]
\end{nprop}

Es decir, es el máximo de las sumas de los valores absolutos de cada $\textbf{columna}$.

\begin{proof} Vamos a demostrar que es $\geq$ y $\leq$, luego se dará la igualdad.\\
Probaremos primero que $ \Vert A \Vert _\infty \geq max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert : i = 1,...,M \right\rbrace $\\
Sea sign(a) := $\left\{ \begin{array}{lcc}
-1 & si & a < 0 \\
1 & si & a \geq 0
\end{array}
\right.$ $\qquad$ $\forall a \in \mathbb{R}$\\
Tenemos que
\[ \left\Vert \left[ sign(a_{11}),...,sign(a_{1N}) \right] ^T \right\Vert _\infty = 1 \qquad \Rightarrow \qquad \Vert A \Vert _\infty \geq \left\Vert A \left[ sign(a_{11}),...,sign(a_{1N}) \right] ^T \right\Vert _\infty \geq \sum_{j=1}^N \vert a_{1j} \vert \]
Hacemos lo mismo con $\left[ sign(a_{i1}),...,sign(a_{iN}) \right] ^T \; \forall i = 2,...,M$ y obtenemos que
\[ \Vert A \Vert _\infty \geq max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert : i = 1,...,M \right\rbrace \]
Ahora probaremos que $ \Vert A \Vert _\infty \leq max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert : i = 1,...,M \right\rbrace $\\
Sea x $\in \mathbb{R}^N$ tal que $\Vert x \Vert _\infty = 1$, entonces:
\[ \Vert Ax \Vert _\infty = \left\Vert 
\begin{bmatrix} 
a_{11} &  \cdots & a_{1N} \\
\vdots & & \vdots 
\\ a_{M1} & \cdots & a_{MN} \\ \end{bmatrix} 
\begin{bmatrix}
x_1 \\
\vdots \\
x_N \\
\end{bmatrix}
\right\Vert _\infty = \left\Vert 
\begin{bmatrix}
\sum_{j=1}^N a_{1j}x_j & ,\ldots , & \sum_{j=1}^N a_{Mj}x_j 
\end{bmatrix} ^T
\right\Vert _\infty = \] \[= max \; \left\lbrace \vert \sum_{j=1}^N a_{ij}x_j \vert : i = 1,...,M \right\rbrace \leq max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert \vert x_j \vert : i = 1,...,M \right\rbrace \leq \] \[ \leq max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert : i = 1,...,M \right\rbrace \] 
\end{proof}

\begin{nprop}
Consideremos la norma $\Vert$·$\Vert _\infty$ en $\mathbb{R}^N$ y en $\mathbb{R}^M$, entonces la norma $\Vert$·$\Vert _\infty$ inducida en $\mathbb{R}^{M \times N}$ es
\[ \Vert A \Vert _\infty = max \; \left\lbrace \sum_{j=1}^N \vert a_{ij} \vert : i = 1,...,N \right\rbrace \qquad \forall A \in \mathbb{R}^{M \times N} \]
\end{nprop}

Es decir, es el máximo de las sumas de los valores absolutos de cada $\textbf{fila}$.\\

Por lo que si ya hemos calculado alguna de estas dos últimas normas, podemos saber la otra sin tener que volver a calcular el máximo, es decir, la relación entre ambas viene en la siguiente proposición.

\begin{nprop}
$\Vert A \Vert _1 = \Vert A^T \Vert _\infty \qquad \forall A \in \mathbb{R}^{M \times N}$
\end{nprop}

Hay que tener en cuenta que $\Vert$ · $\Vert _2$ no induce en $\mathbb{R}^{M \times N}$ Frobenius.\\

Además, para las matrices $\textbf{cuadradas}$ tenemos la siguiente definición.

\begin{ndef}[Radio espectral]
Sea $A \in \mathbb{R}^{N \times N}$, denotaremos como $\textbf{radio espectral de A}$ a:
\[ \rho (A) := max \; \left\lbrace \vert \lambda \vert : \lambda \in \mathbb{C} \wedge det(A - \lambda I) = 0 \right\rbrace \]
\end{ndef}

La siguiente proposición muestra una manera más fácil de calcular la norma euclídea de un vector de $\mathbb{R}^N$, que es calculando la suma de las coordenadas al cuadrado.

\begin{nprop}
$\Vert x \Vert _2 = \sqrt{x^Tx} \qquad \forall x \in \mathbb{R}^N$
\end{nprop}

\begin{ndef}
Sea A $\in \mathbb{R}^{N \times N}$, diremos que A es $\textbf{semidefinida positiva} \Leftrightarrow x^TAx \geq 0 \qquad \forall x \in \mathbb{R}^N $
\end{ndef}

Las matrices semidefinidas positivas tienen la siguiente propiedad.

\begin{nprop}
Sea A $\in \mathbb{R}^{N \times N}$ semidefinida positiva. Si $\lambda$ es un valor propio de A $\Rightarrow \lambda \geq 0$.
\end{nprop}

\begin{proof}
Como $\lambda$ es valor propio de A $\Rightarrow \exists x \in \mathbb{R}^N : x \neq 0 \wedge Ax = \lambda x$\\
Luego\\
\[ 0 \leq x^TAx = x^T \lambda x = \lambda x^Tx = \lambda \Vert x \Vert _2^2 \]
Como x $\neq$ 0 $\Rightarrow 0 \leq \lambda$
\end{proof}

\begin{nprop}
Sea P $\in \mathbb{R}^{N \times N}$ una matriz ortogonal, entonces
\[ \left\lbrace x \in \mathbb{R}^N : \Vert x \Vert _2 = 1 \right\rbrace = \left\lbrace P^Tx : x \in \mathbb{R}^N \wedge \Vert x \Vert _2 = 1 \right\rbrace \]
\end{nprop}

\begin{proof}
Vamos a demostrar la doble inclusión, lo que dará la igualdad.\\
$\textbf{¿ $\supseteq$ ?}$\\
Sea $x \in \mathbb{R}^N : \Vert x \Vert _2 = 1 \; \Rightarrow \;$ ¿ $\Vert P^Tx \Vert _2 = 1 $ ?
\[ \Vert P^Tx \Vert _2 = \sqrt{x^TPP^Tx} = \sqrt{x^Tx} = \Vert x \Vert _2 = 1 \]
$\textbf{¿ $\subseteq$ ?}$\\
\[ 1 = \Vert x \Vert _2 = \sqrt{x^Tx} = \sqrt{x^TIx} = \sqrt{x^TPP^Tx} = \Vert P^Tx \Vert _2 \]
\end{proof}

\begin{nprop}
Si $\lambda _1,..., \lambda _N \geq 0 \Rightarrow sup \; \left\lbrace \sqrt{\sum_{i=1}^N \lambda _iy_i^2} : y \in \mathbb{R}^N \wedge \Vert y \Vert _2 = 1 \right\rbrace = \sqrt{max \; \lambda _i : i = 1,...,N} $
\end{nprop}

\begin{proof}
\end{proof}

Una manera más sencilla de calcular la norma de una matriz es la siguiente.

\begin{nprop}
Sea A $\in \mathbb{R}^{M \times N} \Rightarrow \Vert A \Vert _2 = \sqrt{\rho (A^TA)} $
\end{nprop}

\begin{ndef}[Norma matricial]
Una norma en $\mathbb{R}^{N \times N}$ se dice $\textbf{matricial}$ cuando
\[ \Vert AB \Vert \leq \Vert A \Vert \Vert B \Vert \qquad \forall A,B \in \mathbb{R}^{N \times N} \]
\end{ndef}

Hay que tener en cuenta de que no toda norma en $\mathbb{R}^{N \times N}$ es matricial, por ejemplo:\\
Vamos a utilizar la siguiente norma\\
$\Vert A \Vert := max \; \lbrace \vert a{ij} \vert : i, j = 1,...,N \rbrace \qquad \forall A \in \mathbb{R}^{N \times N}$ \\
Sean A = B = $\begin{bmatrix}
1 & 1 \\
1 & 1 \\
\end{bmatrix}$\\
Luego tenemos que
$2 = \Vert AB \Vert > \Vert A \Vert \Vert B \Vert = 1$\\
Por lo que esta norma no es matricial.\\

\begin{nprop}
Toda norma en $\mathbb{R}^{N \times N}$ inducida por una norma en $\mathbb{R}^N$ es matricial.
\end{nprop}

\begin{proof}
Sea $\Vert$ · $\Vert$ una norma en $\mathbb{R}^{N \times N}$ inducida por una norma en $\mathbb{R}^N$, entonces
\[ \Vert A \Vert := sup \; \left\lbrace \Vert Ax \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace \]
Sabemos que la norma es inducida, luego se cumple que
\[ \Vert Ax \Vert \leq \Vert A \Vert \Vert x \Vert \qquad \forall x \in \mathbb{R}^N \]
Tenemos que probar que $\Vert AB \Vert \leq \Vert A \Vert \Vert B \Vert$\\
\[ \Vert AB \Vert = sup \; \left\lbrace \Vert ABx \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace \leq sup \; \left\lbrace \Vert A \Vert \Vert Bx \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace \leq \] \[ \leq sup \; \left\lbrace \Vert A \Vert \Vert B \Vert \Vert x \Vert : x \in \mathbb{R}^N \wedge \Vert x \Vert = 1 \right\rbrace = \Vert A \Vert \Vert B \Vert \]
\end{proof}

\begin{nth}
$\lim_{n \rightarrow \infty} A^n = 0 \; \Leftrightarrow \; \rho (A) < 1 \qquad \forall A \in \mathbb{R}^{N \times N} $
\end{nth}

Este teorema nos deja dos importantes consecuencias.

\begin{ncor}
Sea A $\in \mathbb{R}^{N \times N}$ una matriz triangular, entonces
\[ \lim_{n \rightarrow \infty} A^n = 0 \quad \Leftrightarrow \quad max \; \lbrace \vert a_{ii} \vert < 1 : i = 1,...,N \rbrace \]
\end{ncor}

\begin{proof}
\end{proof}

\begin{ncor}
Sean N $\geq$ 1, A $\in \mathbb{R}^{N \times N}$ y $\Vert$ · $\Vert$ una norma matricial en $\mathbb{R}^{N \times N}$ tal que $\Vert$A$\Vert$ < 1, entonces $\rho$(A) < 1
\end{ncor}

Hay que tener en cuenta que no se cumple la implicación contraria, por ejemplo:\\
Sea A = $\begin{bmatrix}
0.5 & 500 \\
0 & 0.5 \\
\end{bmatrix}$ $\Rightarrow \rho (A) = 0.5 < 1$ pero $\Vert A \Vert _\infty = 500.5 \geq 1$


\subsection{Problemas bien planteados. Estabilidad}
Nos planteamos el siguiente problema:\\
Sean X e Y subconjuntos no vacíos de sendos espacios normados reales, f: X $\rightarrow$ Y una aplicación, $y_0 \in$ Y. Entonces tenemos que encontrar $x_0 \in X : f(x_0) = y_0$\\
Denotaremos a $x_0$ como la solución que resuelve el problema determinado por f y a $y_0$ los datos, es decir, son números. Si tenemos un conjunto finito de números, usaremos el vector de $\mathbb{R}^N$ o matriz, y si tenemos infinitos datos, usaremos una función.

\begin{ejemplo}
Sean $A \in \mathbb{R}^{M \times N}$ y $y \in \mathbb{R}^M$. Determinar una solución del sistema de ecuaciones lineales cuya matriz de coeficientes sea A y su vector de términos independientes sea y\\
$X = \mathbb{R}^N$, $Y = \mathbb{R}^M$, f(x) = Ax = y
\end{ejemplo}

\begin{ndef}
Un problema está $\textbf{bien planteado}$ cuando es $\textbf{unisolvente}$ y $\textbf{estable}$:
	\begin{nlist}
	\item $\exists ! x_0 \in X : f(x_0) = y_0$.
	\item $x_0$ depende continuamente de los datos $y_0$.
	\end{nlist}
\end{ndef}

En el siguiente ejemplo veremos un problema mal planteado.

\begin{ejemplo}
Sean X := $\mathbb{R}$, Y := $\mathbb{R}_+ \;$ y $\; f(x) := \vert x \vert , \; \forall x \in X$\\
Observamos que este problema no es unisolvente, ya que si $y_0$ = 1 (lo mismo vale $\forall y_0 > 0$) tenemos que f(-1) = 1 = f(1)
\end{ejemplo}

\begin{ndef}[Resolvente]
Denotaremos a la función g como la $\textbf{resolvente}$ de f si g es la inversa de f, para todo y $\in$ Y unisolvente.
\end{ndef}

\begin{ejemplo}
Sean X := $\mathbb{R}$, Y := $\mathbb{R}_+ \;$ y $\; f(x) := e^x , \; \forall x \in X$\\
Entonces este problema es unisolvente, luego tiene resolvente: g(y) = log y, para todo y $\in$ Y.
\end{ejemplo}

Podemos ver la estabilidad de un problema intuitivamente, es decir, a pequeñas perturbaciones de los datos $y_0$ corresponden pequeñas perturbaciones de la solución $x_0$.

\begin{ejemplo}
Consideremos el problema X := [-1,1] =: Y, $\;$
$
f(x) := \left\{ \begin{array}{lcc}
x & si & -1 < x < 1 \\
\\ -x & si & x = \pm 1
\end{array}
\right.
$.\\
Entonces este problema es unisolvente, ya que la función f :  [-1,1] $\rightarrow$ [-1,1] es biyectiva y g = f.\\
Pero para que esté bien planteado, tiene que ser, además, estable. Veamos que no es estable, ya que a pequeñas perturbaciones del dato $y_0$ = -1 (lo mismo con $y_0$ = 1) no corresponden pequeñas perturbaciones de $x_0$ = 1.\\
Sea $y_n := -1 + \frac{1}{n} \rightarrow y_0 = -1$, entonces $g(y_n) = -1 + \frac{1}{n} \rightarrow -1$, ya que $y_n = g(y_n)$.\\
Luego $\vert -1 - x_0 \vert = 2$, es decir, las perturbaciones de $x_0$ son muy grandes.
\end{ejemplo}

Ahora nos planteamos la siguiente cuestión: ¿La estabilidad del problema y la continuidad de la resolvente g tienen algo que ver? Veamos en el siguiente ejemplo que no siempre es así.

\begin{ejemplo}
Consideremos el problema $f : \mathbb{R}_+ \rightarrow \mathbb{R}_+, f(x) := \left( \frac{x}{10} \right) ^10 \; \forall x \geq 0$.\\
Tenemos que este problema es unisolvente y su resolvente es $g : \mathbb{R}_+ \rightarrow \mathbb{R}_+, g(y) = 10y^{\frac{1}{10}} \; \forall y \geq 0$.\\
Tenemos que g es continua, luego pequeños cambios de los datos $y \in \mathbb{R}_+$ nos llevan a cambios cercanos de las correspondientes soluciones $x \in \mathbb{R}_+$ pero no controlados, se aproximan a una velocidad diferente. Vamos a comprobarlo.\\
Tenemos que $y_0 = 0 = x_0$. Sea y un dato próximo a $y_0$, por ejemplo y = $10^{-10}$. Entonces la solución correspondiente es $x = g(y) = 10y^{\frac{1}{10}} = 10 \cdot \left( 10^{-10} \right) ^{\frac{1}{10}} = \frac{10}{10} = 1$. Por lo que tenemos lo siguiente:\\
$\vert y - y_0 \vert = 10^{-10}$, $\vert x - x_0 \vert = 1$.\\
Es decir, el número $10^{-10}$ indica la velocidad con la que se mueven los datos y, en comparacion con el número 1, que indica que los datos x se mueven mucho más rápido, lo que nos lleva a que, aunque g sea continua, la velocidad de aproximación es diferente, luego no hay estabilidad.
\end{ejemplo}

Luego podríamos decir que la estabilidad es una condición que fuerce un control de los valores de las soluciones en función de los datos, de forma que pequeñas perturbaciones de $y_0$ generen perturbaciones pequeñas y controladas de $x_0$.

\begin{ndef}
Sean X e Y subconjuntos no vacíos de sendos espacios normados, $g : Y \rightarrow X$ una aplicación e $y_0 \in Y$. Diremos que g es $\textbf{estable}$ en $y_0$ cuando
 \[ \exists \delta , M > 0 : sup \; \left\lbrace \frac{ \Vert g(y) - g(y_0) \Vert }{ \Vert y - y_0 \Vert } : y \in Y \wedge 0 < \Vert y - y_0 \Vert < \delta \right\rbrace < M \]
y que g es $\textbf{estable}$ si lo es en todos y cada uno de los elementos de Y.
\end{ndef}

\begin{ndef}
Un problema es $\textbf{estable en $y_0 \in$ Y}$ si su resolvente $g : Y \rightarrow X$ lo es en dicho punto, y es $\textbf{estable}$ si lo es en cualquier dato de Y.
\end{ndef}

Habrá mejor comportamiento cuanto más pequeño sea M.

\begin{nprop}
g es estable en $y_0 \Rightarrow$ g es continua en $y_0$ 
\end{nprop}

Veamos que la implicación $\nLeftarrow$ se cumple al tomar la resolvente del ejemplo anterior en 0, o si se quiere, la función raíz cuadrada en 0.\\

\begin{nprop}
Toda función real de variable real de clase $C^1$ es estable.
\end{nprop}

Para demostrar esta proposición hay que usar el Teorema del Valor Medio. Además, el recíproco no es cierto.\\
Sea f una función de variable real de clase $C^1$, vamos a medir su estabilidad en $x_0 \in \mathbb{R}$.\\
Si $x_0f(x_0) \neq 0$, entonces el cociente entre el error relativo cometido cerca de $f(x_0)$ y el error relativo de $x_0$ es:
\[ \left\vert \frac{\frac{f(x) - f(x_0)}{f(x_0)}}{\frac{x - x_0}{x_0}} \right\vert = \left\vert \frac{f(x) - f(x_0)}{x - x_0} \right\vert \left\vert \frac{x_0}{f(x_0)} \right\vert \]
Si $x_0f(x_0) = 0$, entonces los errores absolutos cerca de $f(x_0)$ e $x_0$ es:
\[ \left\vert \frac{f(x) - f(x_0)}{x - x_0} \right\vert \]

De forma precisa:

\begin{ndef}
Dada una función $f \in C^1(\mathbb{R})$ y un punto $x_0 \in \mathbb{R}$, el $\textbf{condicionamiento relativo}$ de f en $x_0$ viene dado por
\[ c(f, x_0) := \left\vert \frac{f'(x_0)x_0}{f(x_0)} \right\vert \]
siempre que $x_0f(x_0) \neq 0$.\\
En caso contrario, el $\textbf{condicionamiento absoluto}$ de f en $x_0$ es
\[ C(f, x_0) := \vert f'(x_0) \vert \]
\end{ndef}

Ídem en funciones reales de variable real definidas en intervalos de $\mathbb{R}$ y de clase $C^1$.\\
Para calcular el condicionamiento en funciones de varias variables, usamos lo siguiente:
\[ c(f,x_0) := \frac{\left\Vert \frac{\partial f}{\partial x} (x_0) \right\Vert \Vert x_0 \Vert}{\Vert f(x_0) \Vert} \]
\[ C(f,x_0) := \left\Vert \frac{\partial f}{\partial x} (x_0) \right\Vert \]

\begin{ndef}
Si en un problema la resolvente s de clase $C^1$ e $y_0 \in Y$, el $\textbf{condicionamiento relativo}$ o $\textbf{absoluto}$ del problema son los de su resolvente en dicho punto.
\end{ndef}

\begin{nota}
Aunque se trata de un concepto bastante ambiguo, suele decirse que una aplicación (o un problema) está $\textbf{bien condicionado}$ en un punto si su condicionamiento es pequeño y en caso contrario, $\textbf{mal condicionado}$.
\end{nota}

\begin{ejemplo}
Sea $f : (0,1) \rightarrow (0, \pi / 2), \; f(x) := arcsen(x)$.\\
Veamos si está bien planteado.\\
La resolvente es $g : (0, \pi / 2) \rightarrow (0,1), \; g(y) := sen(y)$. Luego está bien planteado.\\
Veamos si está bien condicionado.\\ Para ello calculamos el condicionamiento relativo en $y_0 \in (0, \pi / 2)$.\\
\[ c(g,y_0) = \frac{\vert g'(y_0)y_0 \vert }{\vert g(y_0) \vert} = y_0 \frac{cos(y_0)}{sen(y_0)} \]
Como $c(g,y_0) \in (0,1) \Rightarrow$ está bien condicionado.
\end{ejemplo}

\begin{ejemplo}
Sea $A \in \mathbb{R}^{N \times N}$ regular e $y \in \mathbb{R}$, encuentra $x \in \mathbb{R}^N$ : f(x) = y.\\
Sea f(x) = Ax, como A es regular, es unisolvente el problema, luego tiene resolvente $g : \mathbb{R}^N \rightarrow \mathbb{R}^N$, $g(y) := A^{-1}y, \; \; \forall y \in \mathbb{R}^N$\\
Además, el problema es estable en todo $y_0 \in \mathbb{R}^N$, ya que:
\[ \Vert g(y) - g(y_0) \Vert _\infty \leq \Vert A^{-1} \Vert _\infty \Vert y - y_0 \Vert _\infty \]
Ahora calcularemos el condicionamiento relativo.
\[ c(g, y_0) = \frac{\Vert \frac{\partial g}{\partial y} (y_0) \Vert \Vert y_0 \Vert }{\Vert g(y_0)} = \frac{\Vert A^{-1} \Vert \Vert y_0 \Vert }{\Vert A^{-1} y_0 \Vert } = \frac{\Vert A^{-1} \Vert \Vert y_0 \Vert }{\Vert x_0 \Vert } \] 
Luego $Ax_0 = y_0$
\end{ejemplo}

Si $\Vert A^{-1} \Vert$ es grande, $x_0$ e $y_0$ son del mismo orden, entonces el condicionamiento será grande. Observémoslo en el siguiente ejemplo.

\begin{ejemplo}
Sea $A = \begin{bmatrix}
1 & 1 \\
1 & 0.999 \\
\end{bmatrix}$,
un dato $y_0 = \begin{bmatrix}
2 \\
1.999 \\
\end{bmatrix}$
y su solución $x_0 = \begin{bmatrix}
1 \\
1 \\
\end{bmatrix}$\\

Veamos que hay un mal condicionamiento, ya que el condicionamiento es demasiado grande en comparación con el resto. Para ello, calculamos antes la inversa de la matriz A y la norma del máximo de esta, de la solución $x_0$ y del dato $y_0$.

\[ A^{-1} = \begin{bmatrix}
-999 & 1000 \\
1000 & -1000 \\
\end{bmatrix} \]
\[ \Vert A^{-1} \Vert _\infty = 2000, \; \; \Vert x_0 \Vert _\infty = 1, \; \; \Vert y_0 \Vert _\infty = 2 \]
Luego el condicionamiento se calculará con la expresión obtenida anteriormente usando la norma infinito.
\[ c(g, y_0) = \frac{\Vert A^{-1} \Vert _\infty \Vert y_0 \Vert _\infty }{\Vert x_0 \Vert _\infty } = 4000 \]
Como hemos dicho, es un condicionamiento demasiado grande y nos preguntamos ¿cuál es el problema? Pues que al calcular las soluciones de otros datos próximos al inicial, las soluciones obtenidas no son próximas a la solución inicial. Comprobémoslo.\\
Sea y un dato próximo a $y_0$, por ejemplo $y = \begin{bmatrix}
2 \\
1.998 \\
\end{bmatrix}$.
Es próximo, ya que $\Vert y - y_0 \Vert _\infty = 0.001$.\\
Su solución es $x = A^{-1}y = \begin{bmatrix}
0 \\
2 \\
\end{bmatrix}$. No es próxima a la solución inicial, ya que $\Vert x - x_0 \Vert _\infty = 1$\\
La interpretación geométrica de este resultado es: las columnas de A forman base de $\mathbb{R}^{2 \times 2}$, calculamos las coordenadas de y en $y_0$ en dicha base. Un pequeño cambio entre y e $y_0$ genera un cambio grande de coordenadas por ser los vectores de la base casi paralelos.
\end{ejemplo}

Ahora nos planteamos la siguiente pregunta: ¿está globalmente acotado el condicionamiento relativo de un sistema de cuaciones lineales? La respuesta es afirmativa y nos da una idea de cómo de estable es.\\

La siguiente definición será útil para calcular el condicionamiento de una matriz cuando la norma es inducida.

\begin{ndef}
Si $\Vert \cdot \Vert$ denota la norma matricial en $\mathbb{R}^{N \times N}$ inducida por una norma en $\mathbb{R}^N$, que notaremos igualmente como $\Vert \cdot \Vert$, entonces definimos el $\textbf{condicionamiento}$ c(A) de una matriz regular A $\in \mathbb{R}^{N \times N}$ como
\[ c(A) := \Vert A^{-1} \Vert \Vert A \Vert \]
\end{ndef}

Además,
\[ sup \; \left\lbrace c(g, y_0) : y_0 \neq 0 \right\rbrace = sup \; \left\lbrace \frac{\Vert A^{-1} \Vert \Vert y_0 \Vert}{\Vert A^{-1}y_0 \Vert} : y_0 \neq 0 \right\rbrace = sup \; \left\lbrace \frac{\Vert A^{-1} \Vert \Vert Ax_0 \Vert}{\Vert x_0 \Vert} : x_0 \neq 0 \right\rbrace = \Vert A^{-1} \Vert \Vert A \Vert \]

Lo que nos lleva a que si tenemos un mal condicionamiento del sistema, el condicionamiento de la matriz de coeficientes es grande, y viceversa.\\
También tenemos que $c(A) \geq 1$.\\
Si retomamos el ejemplo anterior, usando la norma del máximo en $\mathbb{R}^2$ y la correspondiente norma matricial inducida, obtenemos que c(A) = 4000.

\subsection{Algoritmos. Algoritmo PageRank de Google}

\begin{ndef}[Algoritmo]
Procedimiento que describe de forma precisa, y siempre mediante un número finito de operaciones aritméticas y lógicas elementales, la resolución de un problema.
\end{ndef}

El algoritmo recoge las instrucciones que permiten al ejecutor del mismo resolver completamente el problema. El ejecutor suele ser un ordenador y, de hecho, en la mayoría de los casos, no puede ser una persona.

\begin{ndef}[Análisis Numérico]
Se ocupa de diseñar algoritmos que permitan la resolución efectiva de problemas bien planteados y que involucran números reales.
\end{ndef}

\begin{ndef}[Complejidad de un algoritmo]
Medida del tiempo de ejecución y que suele expresarse en términos de un parámetro asociado al problema.
\end{ndef}

Al desarrollar y analizar un algoritmo obtenemos precisión, estabilidad y efectos de la representación finita de los números reales.

El Algoritmo PageRank de Google mide la relevancia de las páginas web (dominios...) con enlaces en común. Por lo que aquí nuestro problema es calcular la relevancia de una página (P), teniendo en cuenta el número de enlaces de otras páginas a (P) y la importancia de las páginas que establecen enlace con (P).

Nuestro modelo es:\\
Denotaremos al conjunto (finito) de páginas web como 1, 2, ..., $x_N$.\\
Denotaremos a las páginas como 1, 2, ..., i.\\
La relevancia de la página i se representa por $x_i \geq 0$.\\
La página i será más importante que la página j si $x_i > x_j$.

\begin{ejemplo}
%Insertar imagen
La relevancia de cada página será la suma de los cocientes entre la relevancia de la página que entra y los enlaces que salen de esta.
\[ x_1 = \frac{x_3}{3}, \; \; x_2 = \frac{x_1}{3} + \frac{x_3}{3} + \frac{x_4}{2}, \; \; x_3 = \frac{x_1}{3} + \frac{x_1}{3} + \frac{x_4}{2}, \; \; x_4 = \frac{x_1}{3} + x_2 + \frac{x_3}{3} \]

Por lo que siempre obtendremos un sistema compatible indeterminado con un parámetro.\\

Si tomamos $x_4$ como parámetro y le asignamos el valor 10, tenemos lo siguiente:
\[ x_1 = 1.875, \; \; x_2 = 7.5, \; \; x_3 = 5.625, \; \; x_4 = 10 \]
\end{ejemplo}

\section{Errores de redondeo. Iteradores}
Las principales fuentes de error suelen ser equivocarse al elegir el modelo de un problema, la medida de los datos experimentales, error al truncar o al redondear y consecuentemente, al operar, se produce una propagación del error.

\subsection{Sistema posicional y números máquina}
Los ordenadores trabajan con un subconjunto finito de números reales, los $\textbf{números máquina}$, subconjunto que depende de las especificaciones del ordenador.\\

$\textbf{SISTEMAS POSICIONALES DE NUMERACIÓN}$\\
Sea $b \in \mathbb{N}$ la base binaria o decimal, según el estándar ISO/IEC/IEEE60559:2011 del IEEE (Institute of Electrical and Electronic Engineers, www.ieee.org). Sea $s \in$ {0,1} el signo, N,M $\in \mathbb{N} \cup$ {0} y sea $x_k$ la cifra en la posición k tal que $0 \leq x_k < b, \; \; \forall k = -M, ..., N$.\\
La $\textbf{representación posicional}$ de un número real es:
\[ x = (-1)^s \sum_{n=-M}^N x_nb^n \]
\[ x_b := (-1)^s \cdot (x_N...x_1x_0.x_{-1}x_{-2}...x_{-M})_b \]
Además, denominaremos al punto entre $x_0$ y $x_{-1}$ como $\textbf{punto binario}$ o $\textbf{punto decimal}$.

\begin{ejemplo}
Sea $x_{10} = x = 101.11$, su representación posicional es x = 1·$10^2$ + 1·$10^0$ + 1·$10^{-1}$ + 1·$10^{-2}$.\\
Sea $y_2 = (101.11)_2$, su representación posicional es y = 1·$2^2$ + 1·$2^0$ + 1·$2^{-1}$ + 1·$2^{-2}$.
\end{ejemplo}

Variando $M, N \in \mathbb{N} \cup \lbrace 0 \rbrace $ tenemos un subconjunto denso de $\mathbb{R}$. Además, la serie geométrica de razón menor que 1 converge. Se puede extender a infinito:
\[ x = (-1)^s \sum_{- \infty}^N x_nb^n \]
\[ x_b := (-1)^s \cdot (x_N...x_1x_0.x_{-1}x_{-2}...)_b \]

El siguiente ejemplo aprenderemos representar en forma posicional infinita a partir de otra infinita.

\begin{ejemplo}
\[ (0.\stackrel{ \frown}{001}) _2 = \sum_{n=1}^{\infty} \left( 2^{-3} \right) ^n = \frac{1}{7} = \stackrel{ \frown}{0.142857} \]
Para calcular la convergencia de esta serie, como la razón es menor que 1, hemos usado lo siguiente:
\[ \alpha \neq 1, \; n > k \geq 0 \; \; \Rightarrow \sum_{j=k}^n \alpha ^j = \frac{\alpha ^k - \alpha ^{n+1}}{1 - \alpha} \]
En particular, si \[ \vert \alpha \vert < 1 \; \; \Rightarrow  \sum_{j=k}^\infty \alpha ^k = \frac{\alpha ^k}{1 - \alpha} \]
\end{ejemplo}


$\textbf{NÚMEROS MÁQUINA}$

Estos números los utiliza el ordenador. Denotamos a la base como b y a las posiciones de memoria como N.

\begin{ndef}[Representación con punto fijo]
Sea $k \in \mathbb{N}$ fijo, N - k -1 dígitos enteros, k dígitos tras el punto $a_n$ tal que $0 \leq a_n \leq b - 1$. Entonces las N posiciones de memoria son: N = signo + cifras significativas = 1 + N - 1. La representación con punto fijo es la siguiente:
\[ (-1)^s b^{-k} \cdot \sum_{n=0}^{N-2}a_nb^n \; \sim \; (-1)^s \cdot (a_{N-2}...a_k.a_{k-1}...a_0)_b \]
\end{ndef}

\begin{ndef}[Representación con número flotante]
Sea $t \in \mathbb{N}$ el número máximo de dígitos o cifras significativas $a_n$ tales que $0 \leq a_n \leq b - 1$, sea m = $a_1...a_t$ la mantisa tal que $0 \leq m \leq b^t - 1$, sea e $\in \mathbb{Z}$ el exponente, tal que $L \leq e \leq U$ donde L, U $\in \mathbb{Z}$, $L \leq U$. Entonces las N posiciones de memoria son N = signo + cifras significativas + dígitos del exponente = 1 + t + N - t - 1. La representación con punto flotante es:
\[ (-1)^sb^e \cdot \sum_{n=1}^t a_nb^{-n} \; \sim \; (-1)^s \cdot (0.a_1...a_t) \cdot b^e = (-1)^s \cdot m \cdot b^{e-t} \]
\end{ndef}

\begin{ejemplo}
Sea x = -3.4567 y la base b = 10, vamos a representarlo de las dos maneras.
	\begin{nlist}
	\item Representación con punto fijo con k = 4 y N = 6. Luego $x = (-1) \cdot 10^{-4} \cdot (3 \cdot 10^4 + 4 \cdot 10^3 + 5 \cdot 10^2 + 6 \cdot 10 + 7 \cdot 10^0)$ = (-1)(3.4567)
	\item Representación con punto flotante con t = 6 y e = 2. Luego $x = (-1) \cdot 10^2 \cdot (0 \cdot 10^{-1} + 3 \cdot 10^{-2} + 4 \cdot 10^{-3} + 5 \cdot 10^{-4} + 6 \cdot 10^{-5} + 7 \cdot 10^{-6}) = (-1)(0.034567) \cdot 10^2$
	\end{nlist}
\end{ejemplo}

Usualmente hay dos representaciones de punto flotante: la precisión simple y la doble.\\
Si un número no está normalizado, es decir, la cifra significativa principal $a_1$ no es 0, entonces ese número tendrá varias representaciones. Veámoslo en el siguiente ejemplo. 

\begin{ejemplo}
Sean b = 2, t = 3, L = 1, U = 3 y punto flotante para x = 1, entonces:
\[ (0.100) \cdot 2^1 = (0.010) \cdot 2^2 = (0.001) \cdot 2^3 \]
\end{ejemplo}

Para evitar este problema, usaremos la representación normalizada que viene definida a continuación.

\begin{ndef}[Notación del sistema normalizado de punto flotante]
\[ \mathbb{F} (b,t,L,U) := \lbrace 0 \rbrace \cup \left\lbrace (-1)^sb^e \sum_{n=1}^t a_nb^{-n} : s = 0,1, a_1 \neq 0, 0 \leq a_1,...,a_t \leq b - 1, L \leq e \leq U \right\rbrace \]
\end{ndef}

\begin{nprop}
Sean t $\in \mathbb{N}$, L, U $\in \mathbb{Z}$ con L $\leq$ U y x $\in \mathbb{F} (b,t,L,U)$. Entonces:
	\begin{nlist}
	\item -x $\in \mathbb{F} (b,t,L,U)$.
	\item $b^{L-1} \leq \vert x \vert \leq b^U(1 - b^{-t})$.
	\item card$( \mathbb{F} (b,t,L,U)) = 2(b-1)b^{t-1}(U-L+1)+1$.
	\end{nlist}
\end{nprop}

Veamos un importante ejemplo en el que se representan todos los números positivos de un sistema de punto flotante normalizado.

\begin{ejemplo}
Sea $\mathbb{F} (2,4,-1,1)$, entonces los números estrictamente positivos de ese sistema de punto flotante son:

\[ \begin{array}{llll}
(0.1111) \cdot 2 = \frac{15}{8} & (0.1110) \cdot 2 = \frac{7}{4} & (0.1101) \cdot 2 = \frac{13}{8} & (0.1100) \cdot 2 = \frac{3}{2} \\
\\ (0.1011) \cdot 2 = \frac{11}{8} & (0.1010) \cdot 2 = \frac{5}{4} & (0.1001) \cdot 2 = \frac{9}{8} & (0.1000) \cdot 2 = 1 \\
\\ (0.1111) \cdot 2^0 = \frac{15}{16} & (0.1110) \cdot 2^0 = \frac{7}{8} & (0.1101) \cdot 2^0 = \frac{13}{16} & (0.1100) \cdot 2^0 = \frac{3}{4}\\
\\ (0.1011) \cdot 2^0 = \frac{11}{16} & (0.1010) \cdot 2^0 = \frac{5}{8} & (0.1001) \cdot 2^0 = \frac{9}{16} & (0.1000) \cdot 2^0 = \frac{1}{2}\\
\\ (0.1111) \cdot 2^{-1} = \frac{15}{32} & (0.1110) \cdot 2^{-1} = \frac{7}{16} & (0.1101) \cdot 2^{-1} = \frac{13}{32} & (0.1100) \cdot 2^{-1} = \frac{3}{8}\\
\\ (0.1011) \cdot 2^{-1} = \frac{11}{32} & (0.1010) \cdot 2^{-1} = \frac{5}{16} & (0.1001) \cdot 2^{-1} = \frac{9}{32} & (0.1000) \cdot 2^{-1} = \frac{1}{4}
\end{array} \]
Además, podemos calcular lo siguiente:\\
Número de elementos de $\mathbb{F}$(2,4,-1,1) (positivos, negativos y cero):
\[ 2(b-1)b^{t-1}(U-L+1) + 1 = 49 \]
Valores mínimo y máximo (positivos):
\[ b^{L-1} = \frac{1}{4}, \; \; b^U(1-b^{-t}) = \frac{15}{8} \]
\end{ejemplo}

El sistema $\mathbb{F}$(b,t,L,U) no se distribuye uniformemente, aunque sí por bloques.

\begin{ndef}[Épsilon máquina]
Para un sistema de punto flotante $\mathbb{F}$ (b,t,L,U) con $L \leq 1 \leq U$, el $\textbf{épsilon máquina}$, que se escribe como $\varepsilon _M$, es la distancia entre el menor número de $\mathbb{F}$ (b,t,L,U) mayor que 1 y la propia unidad, es decir,
\[ \varepsilon _M := b^{1-t} \]
\end{ndef}

Como $L \leq 1 \leq U \Rightarrow 1 \in \mathbb{F} (b,t,L,U)$ 

\subsection{Redondeo en sistemas de punto flotante y su aritmética}
En este apartado vamos a trabajar sobre el contexto de los números máquina representados en el sistema de punto flotante $\mathbb{F}$(b,t,L,U). Primero, tengamos en cuenta lo siguiente:
	\begin{nlist}
	\item $\mathbb{F}$(b,t,L,U) $\neq \mathbb{R}$.
	\item El resultado de operar con dos números de $\mathbb{F}$ (b,t,L,U) no queda dentro de dicho sistema necesariamente, por ejemplo:
	\[ \frac{1}{4}, \frac{9}{32} \in \mathbb{F} (2,4,-1,1), \; \; pero \; \frac{1}{4} + \frac{9}{32} = \frac{17}{32} \notin \mathbb{F} (2,4,-1,1) \]
	\end{nlist}

\begin{ndef}[Truncatura]
Fijado un sistema de punto flotante concreto $\mathbb{F}$(b,t,L,U) (al que no se hará referencia si no hay lugar a ambigüedad) si x $\in \mathbb{R}$ es el número real 
\[ x =(-1)^sb^e \sum_{n=1}^{\infty} a_nb^{-n} \]
entonces su $\textbf{truncatura}$ (en dicho sistema) es un número de $\mathbb{F}$(b,t,L,U)
\[ tr(x) := (-1)^s \cdot (0.a_1...a_t) \cdot b^e \]
\end{ndef}

\begin{ejemplo}
Sea el sistema de punto flotante $\mathbb{F}$(2,4,-1,1) y el número real x = 1.6875, calcularemos su truncatura. Primero lo pasaremos a la base indicada:
\[ 1.6875 = (0.11011) \cdot 2 \]
Ahora calculamos la truncatura:
\[ tr(1.6875) = (0.1101) \cdot 2 = \frac{13}{8} = 1.625 \]
Es decir, hemos quitado la última cifra, ya que en el número había 5 cifras y según el sistema, el número de cifras es 4. Por último lo hemos pasado a base decimal.
\end{ejemplo}

\begin{ndef}
Para un sistema de punto flotante  $\mathbb{F}$(b,t,L,U), el $\textbf{redondeo}$ del número real
\[ x = (-1)^sb^e \sum_{n=1}^{\infty} a_nb^{-n} \]
es el número de $\mathbb{F}$(b,t,L,U)
\[ rd(x) = tr \left( x + (-1)^s \frac{b}{2} \frac{b^e}{b^{t+1}} \right) \]
\end{ndef}

El redondeo se puede calcular de otra manera.

\begin{nprop}
Para un sistema de punto flotante  $\mathbb{F}$(b,t,L,U), el $\textbf{redondeo}$ del número real
\[ x = (-1)^sb^e \sum_{n=1}^{\infty} a_nb^{-n} \]
es el número de $\mathbb{F}$(b,t,L,U)
\[ rd(x) := (-1)^s \cdot (0.a_1...a_{t-1}r_t) \cdot b^e \]
donde
\[ r_t := \left\{ \begin{array}{lcc}
a_t & si & a_{t+1} < \frac{b}{2}\\
\\ a_t +1 & si & a_{t+1} \geq \frac{b}{2}
\end{array} 
\right. \]
\end{nprop}

\begin{ejemplo}
Sea el sistema de punto flotante  $\mathbb{F}$(2,4,-1,1) y el número real 1.6875, vamos a calcular su redondeo.
	Como la base es 2, vamos a pasarlo a esa base: $1.6875 = (0.11011) \cdot 2$
	Para redondearlo, lo podemos hacer de dos formas.
	\begin{nlist}
	\item Directamente con 5ª cifra tras el punto (t = 4):
	\[ rd(1.6875) = (0.1110) \cdot 2 = \frac{7}{4} = 1.75 \]
	\item Con la definición:
	\[ rd(1.6875) = tr \left( x +(-1)^0 \frac{2}{2} \frac{2}{b^5} \right) = tr ((0.11011) \cdot 2 + (0.00001) \cdot 2) = tr((0.11100) \cdot 2) = (0.1110) \cdot 2 = 1.75 \]
	\end{nlist}
\end{ejemplo}

Los errores de redondeo o truncatura están acotados, se muestran en la siguiente propiedad.

\begin{nprop}
Consideremos el sistema de punto flotante $\mathbb{F}(b,t,L,U)$, con $L \leq e \leq U$ y sea $x = (-1)^sb^e \sum_{n=1}^{\infty} a_nb^{-n} \; \in \mathbb{R}$. Entonces:
	\begin{nlist}
	\item $ \vert x - tr(x) \vert \leq b^{e-t} $
	\item $ \frac{ \vert x - tr(x) \vert }{ \vert x \vert } \leq \varepsilon _M $
	\item $ \vert x - rd(x) \vert \leq \frac{1}{2} b^{e-t}$
	\item $ \frac{\vert x - rd(x) \vert }{\vert x \vert } \leq \frac{\varepsilon _M}{2} $
	\end{nlist}
\end{nprop}

\begin{proof}
$\newline$
	\begin{nlist}
	\item 
	\[ \vert x - tr(x) \vert = b^e \left\vert \sum_{n=t+1}^{\infty} a_nb^{-n} \right\vert \leq b^e(b-1) \sum_{n=t+1}^{\infty} b^{-n} = b^{e-t} \]
	La primera igualdad se da, ya que al restar x y tr(x) el factor común es $b^e$ y se eliminan los términos del 1 hasta la t. En la segunda desigualdad hemos acotado por la cifra más grande. Finalmente, hemos calculado la suma.
	\item Usando (i) y que $a_1 \geq 1$ tenemos que 
	\[ \frac{\vert x - tr(x) \vert}{\vert x \vert} \leq \frac{b^{e-t}}{b^e \sum_{n=1}^{\infty} a_nb^{-n}} \leq \frac{b^{e-t}}{b^e \frac{1}{b}} = b^{1-t} = \varepsilon _M \]
	\item 
	\[ \vert x - rd(x) \vert = b^e \vert (0.a_1...a_ta_{t+1}...) - (0.a_1...r_t) \vert \]
	 ¿$ \vert (0.a_1...a_ta_{t+1}...) - (0.a_1...r_t) \vert \leq \frac{b^{-t}}{2}$ ?
		\item[•] Si $a_{t+1} < \frac{b}{2} \; y \; a_t = r_t$:
		\[ \vert (0.a_1...a_ta_{t+1}...) - (0.a_1...r_t) \vert = (0.0...0a_{t+1}...) \leq (0.0...0 \frac{b}{2}) = \frac{b^{-t}}{2} \]
		\item[•] Si $a_{t+1} \geq \frac{b}{2} \; y \; a_t + 1 = r_t$:
		\[ vert (0.a_1...a_ta_{t+1}...) - (0.a_1...r_t) \vert = \frac{1}{b^t} - \left( \frac{a_{t+1}}{b^{t+1}} + ... \right) \leq \frac{1}{b^t} - \frac{b}{2b^{t+1}} = \frac{b^{-t}}{2} \]
	\item \[ a_1 \geq 1 \Rightarrow \vert x \vert \geq b^eb^{-1} \Rightarrow _{(iii)} \frac{\vert x - rd(x) \vert }{\vert x \vert} \leq \frac{\frac{1}{2}b^{e-t}}{b^eb^{-1}} = \frac{1}{2}b^{1-t} = \frac{\varepsilon _M}{2} \]
	\end{nlist}	
\end{proof}

\begin{ndef}
La cota que aparece en el error relativo del redondeo recibe el nombre de $\textbf{precisión máquina}$ (o $\textbf{unidad de redondeo}$) y se denota por u, es decir,
\[ u := \frac{1}{2}b^{1-t} = \frac{1}{2} \varepsilon _M \]
\end{ndef}

\begin{ncor}
Dados $\mathbb{F} (b,t,L,U)$ y $x \in \mathbb{R}$, con $b^{L-1} \leq \vert x \vert \leq b^U(1 - b^{-t})$, se tiene que
\[ rd(x) = (1 + \mu )x \]
para cierto $\mu \in \mathbb{R}$ tal que $\vert \mu \vert \leq u$
\end{ncor}

\begin{proof}
\[ \vert x - rd(x) \vert \leq u \vert x \vert \; \Leftrightarrow \; x - \vert x \vert u \leq rd(x) \leq x + \vert x \vert u \]
esto es,
\[ rd(x) = x +\kappa \vert x \vert u \]
con $\vert \kappa \vert \leq 1$, es decir,
\[ rd(x) = (1 + \mu )x \]
para cierto $\mu \in \mathbb{R}$ tal que $\vert \mu \vert \leq u$
\end{proof}

Las operaciones con números máquina en $\mathbb{F} (b,t,L,U)$ no son necesariamente internas.

\begin{ndef}
Sea $ \bullet : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ una operación, definimos la $\textbf{operación máquina}$ como
\[ \bullet _M : \mathbb{F} (b,t,L,U) \times \mathbb{F} (b,t,L,U) \rightarrow \mathbb{F} (b,t,L,U) \]
\[ \bullet _M (x,y) := rd(x \bullet y), \; \; (x,y \in \mathbb{F} (b,t,L,U)) \]
\end{ndef}

\begin{ncor}
\[ x,y \in \mathbb{F} (b,t,L,U) \Rightarrow \bullet _M (x,y) = (1 + \mu ) x \bullet y \]
para cierto $\mu \in \mathbb{R}$ tal que $\vert \mu \vert \leq u$
\end{ncor}

Aunque una única operación genere un error pequeño, una sucesión finita de operaciones es susceptible de producir la llamada $\textbf{propagación del error}$, que puede ser considerable. Veámoslo en el siguiente ejemplo.

\begin{ejemplo}
Sea la sucesión $x_n := \int_0^1 x^ne^x dx$, vamos a integrar por partes para obtener una expresión más sencilla.
\[ \int_0^1 x^ne^x dx = x^ne^x ]_0^1 - n \int_0^1 x^{n-1}e^x dx = e - n \int_0^1 x^{n-1}e^x dx \]
Por lo que $x_n = e - nx_{n-1}$.\\
Si $x \in [0,1] \Rightarrow x^ne^x \leq x^{n-1}e^x$.\\
Tenemos que la sucesión $\lbrace x_n \rbrace _{n \geq 0}$ es decreciente y positiva, luego converge a un número $\ell$. Calculamos ese número:
\[ x_{n-1} = \frac{e - x_n}{n} \Rightarrow \ell = 0 \]
Redondeando en el sistema de punto flotante b = 10 y t = 7 obtenemos que:
\[ x_{15} = (0.1604004) \]
\[ x_0 = (0.1718281828) \cdot 10 \]
\[ x_{15} = (0.6004419078) \cdot 10^3 \]
Lo que produce una propagación del error.\\
Ahora vamos a calcular el condicionamiento de una sucesión de funciones directamente relacionadas con la sucesión $\lbrace x_n \rbrace _{n \geq 0}$. Primero tenemos que calcular usando la recurrencia $x_n$, siendo $x_n = f_n (x_0)$ donde $f_n : \mathbb{R} \rightarrow \mathbb{R}$
\[ x_1 = e - x_0 \]
\[ x_2 = e - 2(e - x_0) = -e + 2x_0 \]
\[ x_3 = e - 3(-e + 2x_0) = 4e - 6x_0 \]
Por inducción, para $n \geq 1$:
\[ (-1)^n(n!x_0 - \alpha _n e) \]
con $\alpha _n \in \mathbb{N}$ independiente de $x_0$ (es un valor irrelevante para calcular el condicionamiento).\\
Tenemos que:
\[ x_n = f_n (x_0) \]
\[ f_n (x) = (-1)^n(n!x - \alpha _ne), \; \; (x \in \mathbb{R}) \]
Ahora ya podemos calcular el condicionamiento de $f_n$ en $x_0$:
\[ c(f_n,x_0) = \frac{n!x_0}{x_n} \geq \frac{n!x_0}{x_0} = n! \]
\end{ejemplo}

A continuación vamos a estudiar las operaciones aritméticas elementales y los errores debidos que se producen en estas debidos a truncaturas, redondeos o a cualquier otra circunstancia.\\
Sea x el dato, $\mu _x$ el error relativo para x y (1 + $\mu _x$)x su valor aproximado.
	\begin{nlist}
	\item Suma (igual para la resta).\\
	Sean los datos $x, y \in \mathbb{R}$ tales que $x + y \neq 0$ y sus valores aproximados (1 + $\mu _x$)x, (1 + $\mu _y$)y, entonces el error cometido al realizar la operación suma es:
	\[ (1 + \mu _x)x + (1 + \mu _y)y = x + y + \mu _xx + \mu _yy = (x+y) \left( 1 + \frac{\mu _xx + \mu _yy}{x+y} \right) = (x+y) \left( 1 + \frac{x}{x+y} \mu _x + \frac{y}{x+y} \mu _y \right)  \]
	Luego
	\[ \mu _{x+y} = \frac{x}{x+y} \mu _x + \frac{y}{x+y} \mu _y \]
	Además, si x e y tienen el mismo signo se puede controlar el error relativo:
	\[ \vert \mu _{x+y} \vert \leq \vert \mu _x \vert + \vert \mu _y \vert \]
	Aunque si x e y tienen signos opuestos, $\mu _{x+y}$ puede dispararse si $x + y \approx 0$, generándose un error relativo enorme, conocido como $\textbf{error de cancelación}$. 
	\item Multiplicación.\\
	Los errores relativos de x e y son pequeños.
	\[ (1 + \mu _x)x \cdot (1 + \mu _y)y = (1 + \mu _x + \mu _y + \mu _x \mu _y)xy \approx (1 + \mu _x + \mu _y)xy \]
	Luego
	\[ \mu _{xy} \approx \mu _x + \mu _y \]
	\item División.\\
	Los errores relativos de x e y son pequeños, siendo $y \neq 0$.
	\[ \frac{(1 + \mu _x)x}{(1 + \mu _y)y} = \frac{x}{y}(1 + \mu _x)(1 - \mu _y + \mu _y^2 - \mu _y^3 + \cdots) \approx \frac{x}{y}(1 + \mu _x - \mu _y) \]
	Por lo que
	\[ \mu _{x/y} \approx \mu _x - \mu _y \]
	\end{nlist}	
	
	
\part{Tema 2. Resolución numérica de sistemas de ecuaciones lineales}
En este tema vamos a hacer un tratamiento numérico de los sistemas de ecuaciones lineales. Vamos a usar dos métodos:
	\begin{nlist}
	\item Métodos directos: Gauss, versiones y factorizaciones.
	\item Métodos iterativos: Jacobi y Gauss-Seidel.
	\end{nlist}

\section{Métodos directos: Gauss y versiones, factorización de matrices}
Si tenemos un sistema $N \times N$ unisolvente, con N grande, la regla de Cramer es ineficiente, ya que tenemos un gran número de operaciones elementales. La regla de Cramer hace N + 1 determinantes + N divisiones, lo cual lleva a
\[ (N+1)(N!N-1)+N = (N+1)!N-1 \; \; operaciones \]

\subsection{Sistemas triangulares}
Sean la matriz $U \in \mathbb{R}^{N \times N}$ $\textbf{triangular superior}$ con elementos diagonales no nulos, el vector de incógnitas $x \in \mathbb{R}^N$ y el vector de términos independientes b, tenemos el siguiente sistema triangular: $\textbf{Ux = b}$. Este sistema se resuelve por $\textbf{sustitución hacia atrás}$ y el algoritmo para resolverlo es:
\[ x_i = \frac{1}{u_{ii}} \left( b_i - \sum _{j=i+1}^N u_{ij}x_j \right), \; \; con \; \; i = N,...,1 \]
Sea la matriz $L \in \mathbb{R}^{N \times N}$  $\textbf{triangular inferior}$ con elementos diagonales no nulos, el vector de incógnitas $x \in \mathbb{R}^N$ y el vector de términos independientes b, tenemos el siguiente sistema triangular: $\textbf{Lx = b}$. Este sistema se resuelve por $\textbf{sustitución hacia adelante}$ y el algoritmo para resolverlo es:
\[ x_i = \frac{1}{l_{ii}} \left( b_i - \sum _{j=1}^{i-1} l_{ij}x_j \right), \; \; con \; \; i = 1,...,N \]
		
\subsection{Métodos de Gauss y Gauss-Jordan. Pivotaje}
\begin{nlist}
\item $\textbf{Método de Gauss}$\\
Si tenemos un sistema Ax = b unisolvente, con el método de Gauss obtenemos otro sistema $\textbf{Ux = c}$ equivalente con U triangular superior, que ya hemos visto como se resuelve.\\
Los datos son $N \geq 1, A \in \mathbb{R}^{N \times N}, b \in \mathbb{R}^N$.\\
Sea $A^{(1)}$ := A. Suponemos que $a_{kk}^{(k)} \neq 0 \; con \; k = 1,...,N$ (en caso contrario hemos terminado y no es posible llegar a un sistema triangular equivalente). Definimos recursivamente los multiplicadores:
\[ m_{ik} := \frac{a_{ik}^{(k)}}{a_{kk}^{(k)}}, \; \; con \; \; i = k+1,...,N \]
e
\[ a_{ij}^{(k+1)} := a_{ij}^{(k)} - m_{ik}a{kj}^{(k)}, \; \; con \; \; i = k+1,...,N; \; j = k,...,N \]
\[ b_i^{(k+1)} := b_i^{(k)} - m_{ik}a_{kj}^{(k)}, \; \; con \; \; i = k+1,...,N \]
Por lo que obtenemos un sistema triangular superior equivalente que se resuelve por sustitución hacia atrás:
\[ Ux = c \]
donde $U := A^{(N)}$, $c := b^{(N)}$.
	\begin{ejemplo}
	Sean
	\[ A := 
	\begin{bmatrix}
	1 & 1 & 3 \\
	0.1 & 1 & 1 \\
	1 & 2 & 0 \\
	\end{bmatrix}, \; \;
	b :=
	\begin{bmatrix}
	5 \\
	2.1 \\
	3 \\
	\end{bmatrix} \]
	Entonces vamos a calcular el sistema triangular superior equivalente.
	Tenemos que
	\[ A^{(1)} = A, \; \; b^{(1)} = b \]
			
	\[ A^{(2)} =
	\begin{bmatrix}
	1 & 1 & 3 \\
	0 & 0.9 & 0.7 \\
	0 & 1 & -3 \\
	\end{bmatrix} \; ,
	\; \; \;
	b^{(2)} =
	\begin{bmatrix}
	5 \\
	1.6 \\
	-2 \\
	\end{bmatrix} \]
	
	\[ A^{(3)} =
	\begin{bmatrix}
	1 & 1 & 3 \\
	0 & 0.9 & 0.7 \\
	0 & 0 & -3.7 \\ 
	\end{bmatrix} \; ,
	\; \; \;
	b^{(3)} =
	\begin{bmatrix}
	5 \\
	1.6 \\
	-3.7 \\
	\end{bmatrix} \] %Falta 7 factorial en -3.7
	Ahora resolvemos por sustitución hacia atrás y tenemos que
	\[ x_1 = x_2 = x_3 = 1 \]
	\end{ejemplo}
			
	\begin{nprop}
	Sean $A \in \mathbb{R}^{N \times N}$ una matriz cuadradad y $b \in \mathbb{R}^N$, entonces son equivalentes:
		\begin{nlist}
		\item El correspondiente método de Gauss puede completarse hasta el paso N-ésimo.
		\item Para cada k = 1,..., N la k-ésima submatriz principal de A
		\[ A_k =
		\begin{bmatrix}
		a_{11} & a_{12} & \cdots & a_{1k} \\
		a_{21} & a_{22} & \cdots & a_{2k} \\
		\vdots & \vdots & & \vdots \\
		a_{k1} & a_{k2} & \cdots & a_{kk} \\
		\end{bmatrix} \]
		es regular.
		\end{nlist}
	\end{nprop}
			
Los errores de redondeo afectan al método de Gauss. Además, evitaremos dividir por coeficientes relativamente pequeños y también evitaremos que algún $a_{kk}^{(k)}$ del método de Gauss sea nulo.

\item $\textbf{Método de Gauss con pivotaje}$ (o pivotaje parcial), variante adaptativa de Gauss.\\
En este método se modifica el paso k, por lo que antes de definir los multiplicadores $m_{ik}$, la matriz $A^{(k+1)}$ y el vector $b^{(k+1)}$, intercambiaremos de posición si es necesario dos de las filas k,...,N de la matriz $A^{(k)}$ de forma que el elemento del vector que tiene mayor valor absoluto sea $a_{kk}^{(k)}$.
\[
\begin{bmatrix}
a_{kk}^{(k)} \\
\vdots \\
a_{Nk}^{(k)} \\
\end{bmatrix}
\; \; \rightsquigarrow \; \;
\begin{bmatrix}
a_{11}^{(1)} & a_{12}^{(1)} & \cdots & \cdots & \cdots & \cdots & a_{1N}^{(1)} \\
0 & a_{22}^{(2)} & \cdots & \cdots & \cdots & \cdots & a_{2N}^{(2)} \\
\vdots & & \ddots & & & & \vdots \\
0 & \cdots & \cdots & 0 & a_{kk}^{(k)} & \cdots & a_{kN}^{(k)} \\
\vdots & & & \vdots & \vdots & & \vdots \\
0 & \cdots & \cdots & 0 & a_{Nk}^{(k)} & \cdots & a_{NN}^{(k)}\\
\end{bmatrix}
\]
Finalmente, el sistema equivalente al de partida se resuelve por sustitución hacia atrás.
	\begin{nota}
	El método de Gauss con pivotaje tiene sentido hasta el último paso N-ésimo $\Leftrightarrow$ la matriz de coeficientes A es regular (el sistema es compatible determinado).
	\end{nota}
			
	\begin{nota}
	Otra variante adaptativa del método de Gauss, $\textbf{pivotaje total o completo}$, no solo
reordena las filas k,..., N de $A^{(k)}$, sino también las columnas k,..., N de forma
que el elemento de la submatriz de dicha matriz correspondiente a las
mencionadas filas y columnas tenga a $a_{kk}^{(k)}$ como el elemento de mayor valor absoluto. Sin embargo, requiere un mayor número de operaciones.
	\end{nota}
		
\item $\textbf{Método de Gauss-Jordan}$.\\
Para terminar con las variantes del método de Gauss, mencionemos el llamado $\textbf{método}$ $\textbf{de}$ $\textbf{Gauss-Jordan}$, que consiste en hacer ceros no solo debajo de $a_{kk}^{(k)}$ sino
también por encima, con el mismo tipo de fórmula. Sin embargo, su coste en operaciones aritméticas es superior.
\end{nlist}
		
\subsection{Métodos de factorización}
Los métodos de factorización se usan cuando tenmos sistemas con la misma matriz de coeficientes, para resolverlos más rápido.
		
\begin{ndef}[Factorización LU]
Sea Ax = b un sistema compatible determinado, sea L una matriz triangular inferior y U una matriz triangular superior, entonces:
\[ \textbf{A = LU} \]
\end{ndef}

Como A = LU $\Rightarrow$ LUx = b. Por lo que para resolver este sistema, se siguen los siguientes paso:
		
\begin{nlist}
\item Calculamos las matrices L y U.
\item Sea $\textbf{y := Ux}$, entonces tenemos que resolver y de $\textbf{Ly = b}$ por sustitución hacia delante.
\item Por último, resolvemos x de $\textbf{Ux = y}$ por sustitución hacia atrás.
\end{nlist}
			
\begin{ejemplo}
Sean
\[ A = \begin{bmatrix}
1 & 3 & -1 \\
2 & 8 & 4 \\
-1 & 3 & 4 \\
\end{bmatrix} 
\; , \; \;
b = \begin{bmatrix}
-1 \\
2 \\
0 \\
\end{bmatrix}
\]
Primero calculamos las matrices L y U. Como A = LU, entonces:
\[ L = \begin{bmatrix}
1 & 0 & 0 \\
2 & 1 & 0 \\
-1 & 3 & 1 \\
\end{bmatrix}
\; , \; \;
U = \begin{bmatrix}
1 & 3 & -1 \\
0 & 2 & 6 \\
0 & 0 & -15 \\
\end{bmatrix}
\]
Ahora tenemos que resolver el sistema Ly = b.
\[ \begin{bmatrix}
1 & 0 & 0 \\
2 & 1 & 0 \\
-1 & 3 & 1 \\
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\
y_3 \\
\end{bmatrix}
\; = \;
\begin{bmatrix}
-1 \\
2 \\
0 \\
\end{bmatrix}
\]
Lo resolvemos por sustitución hacia adelante y obtenemos que $y_1 = -1, \; y_2 = 4, \; y_3 = -13$.\\
Ahora resolvemos el sistema Ux = y.
\[ 
\begin{bmatrix}
1 & 3 & -1 \\
0 & 2 & 6 \\
0 & 0 & -15 \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}
\; = \;
\begin{bmatrix}
-1 \\
4 \\
-13 \\
\end{bmatrix}
\]
Lo resolvemos por sustitución hacia atrás y obtenemos que $x_1 = \frac{5}{3}, \; x_2 = \frac{-3}{5}, \; x_3 = \frac{13}{15}$. Que es la solución del sistema inicial Ax = b.
\end{ejemplo}

Tenemos que tener en cuenta que no siempre podemos obtener una factorización tipo LU para una matriz regular. Solo se podrá si $a_{11} = l_{11}u_{11} \neq 0$.

\begin{nprop}
Sean $N \geq 1$, $A \in \mathbb{R}^{N \times N}$, $b \in \mathbb{R}^N$ y supongamos que aplicando el método de Gauss al sistema Ax = b se obtiene una matriz triangular superior $A^{(N)}$ y un vector $b^{(N)}$ de forma que el sistema $A^{(N)}x = b^{(N)}$ es equivalente al de partida. Entonces
\[ A = LU,\]
siendo
\[ U = A^{(N)} \]
\[ L = \begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
m_{21} & 1 & 0 & \cdots & 0 \\
m_{31} & m_{32} & 1 & \cdots & 0 \\
\vdots &  \vdots & \vdots & \ddots & \vdots \\
m_{N1} & m_{N2} & m_{N3} & \cdots & 1 \\
\end{bmatrix}, \]
donde los coeficientes de la parte inferior de L son los multiplicadores del método de Gauss definidos recursivamente.
\end{nprop}			
			
\begin{nth}
Sea $A \in \mathbb{R}^{N \times N}$ una matriz regular. Entonces equivalen
	\begin{nlist}
	\item Para cualquier $b \in \mathbb{R}^N$, el método de Gauss para el correspondiente sistema de ecuaciones lineales puede completarse hasta el paso N-ésimo.
	\item A admite una factorización LU.
	\item Las N submatrices principales de A son regulares.
	\end{nlist}
\end{nth}
			
\begin{nth}
Sea A una matriz regular, entonces:
	\begin{nlist}
	\item El método de Gauss con pivotaje es factible, para cualquier sistema de ecuaciones lineales que tenga a A por matriz de coeficientes.
	\item Salvo la eventual permutación de algunas de sus filas, A admite una factorización LU.
	\end{nlist}
\end{nth}
			
\begin{ndef}[Factorización de Doolittle]
			En el sistema LUx = b, los coeficientes de la diagonal de L son 1, es decir
\[ l_{11} = \cdots = l_{NN} = 1 \]
\end{ndef}
			
A la hora de programar la factorización de Doolittle, el algoritmo es:\\
Sea i = 1,...,N, entonces
\[ u_{ij} = a_{ij} - \sum_{k=1}^{i-1} l_{ik}u_{kj}, \; \; con \; j = i,...,N \]
Y supuesto que $u_{ii} \neq 0$
\[ l_{ji} = \frac{1}{u_{ii}} \left( a_{ji} - \sum_{k=1}^{i-1} l_{jk}u_{ki} \right), \; \; con \; j = i+1,...,N \]			
			
\begin{ndef}[Factorización de Crout]
En el sistema LUx = b, los coeficientes de la diagonal de U son 1, es decir
\[ u_{11} = \cdots = u_{NN} = 1 \]
\end{ndef}
			
\begin{ndef}[Matriz definida positiva]
Sea $A \in \mathbb{R}^{N \times N}$, se dice que es $\textbf{definida positiva}$ si
\[ x^TAx > 0, \; \; \; \forall x \in \mathbb{R}^N \backslash \lbrace 0 \rbrace \]
\end{ndef}
			
La factorización LU en matrices simétricas definidas positivas es válida siempre. Además, $L = U^T$.
			
\begin{ndef}[Factorización tipo Cholesky]
Sea $A \in \mathbb{R}^{N \times N}$ una matriz simétrica y definida positiva. Entonces existe una única matriz triangular superior $U \in \mathbb{R}^{N \times N}$ con coeficientes positivos en su diagonal principal y de forma que
\[ A = U^TU \]
\end{ndef}
			
A la hora de programar la factorización tipo Cholesky, el algoritmo es:\\
Para todo j = 1,...,N
\[ i = 1,...,j-1 \; \Rightarrow \; u_{ij} = \frac{1}{u_{ii}} \left( a_{ij} - \sum_{k=1}^{i-1} u_{ki}u_{kj} \right) \]
\[ u_{jj} = \sqrt{a_{jj} - \sum_{k=1}^{j-1}u_{kj}^2} \]
Además, se eliminan los elementos de debajo de la diagonal principal, ya que hay simetría.
			
\begin{nprop}
Una matriz cuadrada A es simétrica y definida positiva si, y solo si, admite una factorización tipo Cholesky.
\end{nprop}
		
\section{Métodos iterativos: métodos de Jacobi y Gauss-Seidel}
Los métodos iterativos nos dan la solución de un sistema de ecuaciones lineales cuadrado y compatible determinado como límite de una sucesión. Cada término de la sucesión se genera de forma recursiva a partir del anterior, los cuales reciben el nombre de iteradores. Además, cuando tenemos sistemas de grandes dimensiones y la matriz de coeficientes es dispersa (número de coeficientes no nulos relativamente pequeño), tenemos problemas prácticos: análisis matricial de estructuras, método de elementos finitos...

\subsection{Métodos iterativos: convergencia y consistencia}
Sea la matriz $A \in \mathbb{R}^{N \times N}$ regular y un vector $b \in \mathbb{R}^N$, entonces tenemos un sistema de ecuaciones lineales cuadrado y unisolvente Ax = b.

\begin{ndef}[Método iterativo]
Sean $B \in \mathbb{R}^{N \times N}$, $x_0,c \in \mathbb{R}^N$, entonces se define al $\textbf{método iterativo}$ como:
\[ \left\{ \begin{array}{c}
x_0 \; dado \\
n \geq 1 \Rightarrow x_n = Bx_{n-1}+c
\end{array}
\right. \]
\end{ndef}

\begin{ndef}[Consistencia]
Sea x la solución del sistema, entonces
\[ \lim_{n \rightarrow \infty}x_n = x \]
\[ \Downarrow (u \in \mathbb{R}^N \mapsto Bu + c \in \mathbb{R}^N continua) \]
\[ \textbf{x = Bx + c} \]
Entonces se dice que hay $\textbf{consistencia}$ del método con el sistema.
\end{ndef}

\begin{nprop}
La consistencia del método iterativo con el sistema equivale a
\[ c = (I-B)A^{-1}b \]
\end{nprop}

\begin{nprop}
Si el método iterativo converge a la solución del sistema, entonces el método es consistente con el sistema.
\end{nprop}

El recíproco es falso. Veámoslo en el siguiente ejemplo.

\begin{ejemplo} Sea $I \in \mathbb{R}^{N \times N}$ la matriz identidad de orden N y sea $b \in \mathbb{R}^N$. Dados el sistema de ecuaciones lineales y el método iterativo
\[
2Ix = b y \qquad \left\{ \begin{array}{c}
x_0 \; dado \\
n \geq 1 \Rightarrow x_n = -x_{n-1}+c \; ,
\end{array}
\right.
\]
Entonces el método es consistente con el sistema si, y solo si,
\[ \textbf{c = b} \]
y converge a la solución del sistema cuando, y solo cuando,
\[ c = 2x_0 \]
\end{ejemplo}

El pseudorrecíproco se da en la siguiente proposición.

\begin{nprop}
Supongamos que $N \geq 1$, $A,B \in \mathbb{R}^{N \times N}$ con A regular, $x_0,b,c \in \mathbb{R}^N$ y que el método iterativo
\[ \left\{ \begin{array}{c}
x_0 \; dado \\
n \geq 1 \Rightarrow x_n = Bx_{n-1}+c
\end{array}
\right. \]
es consistente con el sistema unisolvente Ax = b. Entonces método iterativo converge a la solución del sistema cualquiera sea $x_0 \in \mathbb{R}^N$ $\Leftrightarrow$ $\rho (B) < 1$.
\end{nprop}

	\begin{proof}
	Demostremos primero la consistencia. Sea $n \geq 1$, entonces
	\[ x_n - x = Bx_{n-1} + c - x = Bx_{n-1} + (I - B)x - x = B(x_{n-1} - x) \]
	Resursivamente obtenemos que $x_n - x = B^n (x_0 - x)$. (2)\\
	¿ $\Rightarrow$ ?\\
	Basándonos en la relación de recurrencia (2) y la convergencia para todo $x_0 \in \mathbb{R}^N$, tenemos que
	\[ \lim_{n \rightarrow \infty} B^n(x_0 - x) 0 
	\; \Leftrightarrow \; 
	\lim_{n \rightarrow \infty} B^n = 0
	\; \Leftrightarrow \;
	\rho (B) < 1 \]
	
	¿ $\Leftarrow$ ?\\
	Sea $x_0 \in \mathbb{R}^N$, como $\rho (B) < 1 \Rightarrow \lim_{n \rightarrow \infty} B^n = 0$.\\
	Usando la relación de recurrencia (2), $\Vert \cdot \Vert$ en $\mathbb{R}^N$ y su matricial inducida en $\mathbb{R}^{N \times N}$, notada de la misma forma, y con n $\geq 1$, entonces
	\[ \Vert x_n - x \Vert = \Vert B^n(x_0-x) \Vert \leq \Vert B^n \Vert \Vert x_0 - x \Vert \]
	Por lo que $\lim_{n \rightarrow \infty}x_n = x$
	\end{proof}
	
\begin{nota}
Como hemos dicho, el método iterativo converge para todo $x_0 \in \mathbb{R}^N$. Aunque esto es falso si el método iterativo es consistente con el sistema y no converge para todo $x_0 \in \mathbb{R}^N$. Por ejemplo, el sistema del ejemplo anterior\\
b := 0 =: c $\Rightarrow$ consistencia\\
Además, hay convergencia solo cuando $c = 2x_0 \Leftrightarrow x_0 = 0$ y $\rho (B) = \rho (-I) = 1$.
\end{nota}

\subsection{Generación de métodos iterativos. Jacobi y Gauss-Seidel}
La proposición anterior nos orienta al procedimiento del diseño de métodos iterativos. Por lo que son automáticamente consistentes, ya que elimina el grave problema que surge al comprobar dicha condición: hay que conocer a priori la solución del sistema... ¡que es justo lo que se pretende aproximar!

Sea la matriz $A \in \mathbb{R}^{N \times N}$ regular y el vector $b \in \mathbb{R}^N$, entonces tenemos un sistema unisolvente Ax = b. Sea $\textbf{A =  M - N}$, por lo que M es regular (siempre es posible por ser A regular) y  con N no nula. Tenemos que
\[ Ax = b \Leftrightarrow (M-N)x = b \Leftrightarrow x = M^{-1}Nx + M^{-1}b \]
Por lo que sugiere que $\textbf{B = M} ^{ \textbf{-1} } \textbf{N}$ y $\textbf{c = M} ^{\textbf{-1} } \textbf{b}$, tenemos el siguiente método iterativo

\[ \left\{ \begin{array}{c}
x_0 \; dado \\
n \geq 1 \Rightarrow x_n = M^{-1}Nx_{n-1} + M^{-1}b
\end{array}
\right. \]

que es consistente con el sistema
\[ (I-B)A^{-1}b = (I-M^{-1}N)A^{-1}b = (M^{-1}M-M^{-1}N)A^{-1}b = M^{-1}(M-N)A^{-1}b = M^{-1}b = c \]


\begin{ncor}
Sean $A, M, N \in \mathbb{R}^{N \times N}$ con A y M regulares de forma que $\textbf{A = M - N}$ y sean $b, x_0 \in \mathbb{R}^N$. Consideremos el sistema $$Ax = b$$ y el método iterativo
\[ \left\{ \begin{array}{c}
x_0 \; dado \\
n \geq 1 \Rightarrow x_n = M^{-1}Nx_{n-1} + M^{-1}b
\end{array}
\right. \]
Entonces el método iterativo converge a la solución del sistema, cualquiera sea $x_0 \in \mathbb{R}^N \Leftrightarrow \rho (M^{-1}N) < 1$.
\end{ncor}

En resumen, todo método iterativo convergente es de esta forma:

\begin{nprop}
Sean $A, B \in \mathbb{R}^{N \times N}$, con A regular, sean $b, c \in \mathbb{R}^N$ y consideremos el sistema de ecuaciones lineales $$Ax = b$$ y el método iterativo
\[ \left\{ \begin{array}{c}
x_0 \; dado \\
n \geq 1 \Rightarrow x_n = Bx_{n-1} + c
\end{array}
\right. \]
que supondremos que converge hacia la solución del sistema para cualquier estimación inicial $x_0 \in \mathbb{R}^N$. Entonces existe una descomposición de la matriz de coeficientes
\[ \textbf{A = M - N} \]
con $M, N \in \mathbb{R}^{N \times N}$ y M regular, tales que
\[ \textbf{B = M}^{-1} \textbf{N} \; \; c = \textbf{B = M}^{-1} \textbf{b} \]
\end{nprop}

Por lo que tenemos el siguiente método iterativo

\[ \left\{ \begin{array}{c}
x_0 \; dado \\
n \geq 1 \Rightarrow x_n = M^{-1}Nx_{n-1} + M^{-1}b
\end{array}
\right. \]
con M regular y A = M - N.

Aunque es un poco complejo calcular la inversa de M y luego multiplicarla por otra matriz, por lo que trabajaremos sobre otro método iterativo equivalente, que consiste en multiplicar por M a ambos lados de la igualdad:

\[ \left\{ \begin{array}{c}
x_0 \; dado \\
n \geq 1 \Rightarrow Mx_n = Nx_{n-1} + b
\end{array}
\right. \]

Para que converja, tenemos que calcular igualmente si $\rho (M^{-1}N) < 1$. Además, la iteración $x_n$ es la solución del sistema resoluble sin un alto coste operativo, ya que M es triangular.
\[ Mx_n = Nx_{n-1} + b \]

Ahora vamos a poner en práctica estos conceptos. Los métodos iterativos más populares son: $\textbf{Jacobi}$ y $\textbf{Gauss-Seidel}$. Definamos primero algunas matrices a partir de la matriz de coeficientes A.

Sea $A \in \mathbb{R}^{N \times N}$ regular, definimos las matrices diagonales y triangulares:

\[ D := \begin{bmatrix}
a_{11} & 0 & \cdots & 0 \\
0 & a_{22} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & a_{NN} \\
\end{bmatrix} \]

\[ E := \begin{bmatrix}
0 & 0 & 0 & \cdots & 0 \\
-a_{21} & 0 & 0 & \cdots & 0 \\
-a_{31} & -a_{32} & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & & \vdots \\
-a_{N1} & -a_{N2} & -a_{N3} & \cdots & 0 \\
\end{bmatrix} \]

\[ F := \begin{bmatrix}
0 & \cdots & -a_{12} & -a_{1 \; N-1} & -a_{1N} \\
\vdots & & \vdots & \vdots & \vdots \\
0 & \cdots & 0 & -a_{N-2 \; N-1} & -a_{N-2 \; N} \\
0 & \cdots & 0 & 0 & -a_{N-1 \; N} \\
0 & \cdots & 0 & 0 & 0 \\
\end{bmatrix} \]

Observemos que la matriz A verifica la hipótesis adicional: $a_{11}a_{22}...a_{NN} \neq 0$.

Bajo esta notación definimos los siguientes métodos iterativos.

\begin{ndef}[Método de Jacobi]
\[ A = M - N \; \; con \; \; \textbf{M := D} \; y \; \textbf{N := E + F} \]
Por lo que tenemos que
\[ \left\{ \begin{array}{c}
x_0 \; dado \\
n \geq 1 \Rightarrow Dx_n = (E+F)x_{n-1} + b
\end{array}
\right. \]
Expresado en coordenadas:
\[
x_0 = \left[ x_{01},...,x_{0N} \right] ^T
\]
\[
i = 1,...,N \Rightarrow x_{ni} = \frac{1}{a_{ii}} \left( b_i - \sum_{j=1 \wedge j \neq i}^N a_{ij}x_{n-1 \; j} \right)
\]

\end{ndef}

\begin{ndef}[Método de Gauss-Seidel]
\[ A = M - N \; \; con \; \; \textbf{M := D - E} \; y \; \textbf{N := F} \]
Por lo que tenemos que
\[ \left\{ \begin{array}{c}
x_0 \; dado \\
n \geq 1 \Rightarrow (D-E)x_n = Fx_{n-1} + b
\end{array}
\right. \]
Expresado en coordenadas:
\[
x_0 = \left[ x_{01},...,x_{0N} \right] ^T
\]
\[
i = 1,...,N \Rightarrow x_{ni} = \frac{1}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij}x_{nj} - \sum_{j=i+1}^N a_{ij}x_{n-1 \; j} \right)
\]
\end{ndef}

La similitud que tienen estos métodos es, que descritos a partir del esquema obtenido al despejar en Ax = b $x_1$ de la primera ecuación, $x_2$ de la segunda y así hasta $x_N$ de la N-ésima

\[ \left\{ \begin{array}{c}
x_1 = \frac{1}{a_{11}} (b_1 - a_{12}x_2 - a_{13}x_3 - \cdots - a_{1N}x_N) \\
x_2 = \frac{1}{a_{22}} (b_2 - a_{21}x_1 - a_{23}x_3 - \cdots - a_{2N}x_N) \\
\vdots \\
x_N = \frac{1}{a_{NN}} (b_N - a_{N1}x_1 - a_{N2}x_2 - \cdots - a_{N \; N-1}{N-1})
\end{array}
\right. \]
En el método de Jacobi: a la derecha las coordenadas de una iteración para obtener a la izquierda la siguiente.

Gauss-Seidel: a la derecha las coordenadas de una iteración y las que se acaban de hallar de la siguiente más arriba para determinar las de la siguiente a la izquierda.

La diferencia es que en Jacobi el vector de cada iteración se calcula a partir del anterior, y en cambio, en Gauss-Seidel, el vector en cada iteración usa las coordenadas que ya se han calculado en la iteración actual.

Ahora nos preguntamos: ¿es el método de Gauss-Seidel más eficiente que el método de Jacobi? Aunque esto no se cumple siempre, veámoslo en un ejemplo.

\begin{ejemplo}
Tenemos el siguiente sistema de ecuaciones
\[ \begin{bmatrix}
2 & 1 & 3 \\
-1 & 3 & 2 \\
1 & 4 & 5 \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix} 
\; = \;
\begin{bmatrix}
9 \\
-1 \\
11 \\
\end{bmatrix}
\]
Por lo que su solución es $x_1 = 1$, $x_2 = -2$, $x_3 = 3$.

Usemos los métodos de Jacobi y Gauss-Seidel para ver si convergen a esta solución. Supongamos una estimación inicial $x_0 = \left[ 0,0,0 \right] ^T$

Despejamos $x_i$ de la ecuación i-ésima (i = 1,2,3) y obtenemos que
\[ \left\{ \begin{array}{c}
x_1 = 4.5 - 0.5x_2 - 1.5x_3 \\
x_2 = - \frac{1}{3} + \frac{1}{3} x_1 - \frac{2}{3} x_3 \\
x_3 = \frac{11}{6} - \frac{1}{6} x_1 - \frac{3}{2} x_2
\end{array}
\right. \]

Eso se hace para ambos métodos, pero en el siguiente paso ya distinguimos cada método.
	\begin{nlist}
	\item[•] Jacobi:\\
	el algoritmo parte de $x_0$ y para cada $n \geq 1$
	\[ \left\{ \begin{array}{c}
	x_{n1} = 4.5 - 0.5x_{n-1 \; 2} - 1.5x_{n-1 \; 3} \\
	x_{n2} = - \frac{1}{3} + \frac{1}{3} x_{n-1 \; 1} - \frac{2}{3} x_{n-1 \; 3} \\
	x_{n3} = \frac{11}{6} - \frac{1}{6} x_{n-1 \; 1} - \frac{3}{2} x_{n-1 \; 2}
	\end{array}
	\right. \]
	obteniendo (truncando):
	\[
	\left\{ \begin{array}{lll}
	x_{01} = 0 & x_{02} = 0 & x_{03} = 0 \\
	x_{11} = 4.5 & x_{12} = -0.333 & x_{13} = 1.833 \\
	\cdots & \cdots & \cdots \\
	x_{20 \; 1} = 1.308 & x_{20 \; 2} = -1.67 & x_{20 \; 3} = 2.702
	\end{array}
	\right.
	\]	
	\item[•] Gauss-Seidel:\\
	con la estimación inicial $x_0$, para todo $n \geq 1$ tenemos que
	\[ \left\{ \begin{array}{c}
	x_{n1} = 4.5 - 0.5x_{n-1 \; 2} - 1.5x_{n-1 \; 3} \\
	x_{n2} = - \frac{1}{3} + \frac{1}{3} x_{n1} - \frac{2}{3} x_{n-1 \; 3} \\
	x_{n3} = \frac{11}{6} - \frac{1}{6} x_{n1} - \frac{3}{2} x_{n2}
	\end{array}
	\right. \]
	con lo que se generan (truncando) los datos numéricos
	\[
	\left\{ \begin{array}{lll}
	x_{01} = 0 & x_{02} = 0 & x_{03} = 0 \\
	x_{11} = 4.5 & x_{12} = 1.166 & x_{13} = 0.305 \\
	\cdots & \cdots & \cdots \\
	x_{20 \; 1} = 1.035 & x_{20 \; 2} = -1.961 & x_{20 \; 3} = 2.968
	\end{array}
	\right.
	\]	
	\end{nlist}
Por lo que en este sistema Gauss-Seidel es más eficiente, pero no en general.
\end{ejemplo}

Hay que tener en cuenta que la velocidad de convergencia de Jacobi es independiente de la de Gauss-Seidel, y viceversa. Estudiemos cada una.

\begin{nprop}
Cuando menor sea $\rho (M^{-1}N)$, mejor será la convergencia de la sucesión de iteradores hacia la solución del sistema.
\end{nprop}

\begin{ejemplo}
Tenemos dos matrices
\[
A_1 = \begin{bmatrix}
4 & 1 & 1 \\
2 & -9 & 0 \\
0 & -8 & -6 \\
\end{bmatrix}
\; , \; \;
A_2 = \begin{bmatrix}
7 & 6 & 9 \\
4 & 5 & -4 \\
-7 & -3 & 8 \\
\end{bmatrix}
\; , \; \;
A_3 = \begin{bmatrix}
3 & 0 & 4 \\
7 & 1 & 2 \\
-1 & 1 & 9 \\
\end{bmatrix}
\]
	\begin{nlist}
	\item[•] $A_1$:
	\[ \rho _{Jacobi} := \rho (D^{-1}(E+F)) = 0.444 \]
	\[ \rho _{Gauss-Seidel} := \rho ((D-E)^{-1}F) = 0.019 \]
	En este caso, Gauss-Seidel es más eficiente.
	\item[•] $A_2$:
	\[ \rho _{Jacobi} := \rho (D^{-1}(E+F)) = 0.641 \]
	\[ \rho _{Gauss-Seidel} := \rho ((D-E)^{-1}F) = 0.775 \]
	En este caso, Jacobi es más eficiente.
	\item[•] $A_3$:
	\[ \rho _{Jacobi} := \rho (D^{-1}(E+F)) = 1.037 \]
	\[ \rho _{Gauss-Seidel} := \rho ((D-E)^{-1}F) = 0.963 \]
	En este caso, Jacobi no converge, ya que $\rho _{Jacobi} \geq 1$. Solo converge Gauss-Seidel.
	\end{nlist}
\end{ejemplo}

\begin{ndef}[Diagonalmente estrictamente dominante]
Sea la matriz $A \in \mathbb{R}^{N \times N}$, diremos que A es diagonalmente estrictamente dominante si
\[ i = 1,...,N \Rightarrow \sum_{j=1 \wedge j \neq i}^N \vert a_{ij} \vert \; < \vert a_{ii} \vert \]
\end{ndef}

Los métodos de Jacobi y de Gauss-Seidel convergen para cualquier sistema de ecuaciones lineales que tenga a A por matriz de coeficientes a su solución, cualquiera sea la elección de la estimación inicial.

\begin{nprop}
Sea $A \in \mathbb{R}^{N \times N}$ una matriz diagonalmente estrictamente dominante. Entonces los métodos de Jacobi y de Gauss–Seidel, para todo sistema de ecuaciones lineales que tenga como matriz de coeficientes A, convergen hacia su solución, independientemente de la estimación inicial que se fije.
\end{nprop}

El recíproco falso: basta considerar cualquier sistema en el que la matriz de
coeficientes sea la matriz $A_1 (o A_2)$ del penúltimo ejemplo.

\begin{nota}
Tenemos que tener en cuenta lo siguiente:
	\begin{nlist}
	\item[•] Si se aplica al sistema de partida una transformación elemental tan simple como intercambiar de posición dos ecuaciones y se usa el mismo método iterativo con los dos sistemas, uno puede converger y otro no. La idea es que este tipo de transformación elemental no solo puede modificar claramente el hecho de que la matriz de coeficientes sea diagonalmente estrictamente dominante (que es una condición suficiente para la convergencia de Jacobi y Gauss–Seidel) sino que además puede cambiar el radio espectral.
	\item[•] El número de operaciones que hay que realizar para pasar de una iteración a la siguiente en los métodos de Jacobi y Gauss–Seidel es de $N^2$ para un sistema de N ecuaciones y N incógnitas. Por tanto, si N es grande, requiere en principio menos operaciones que los directos. Además, y a diferencia de estos últimos, aprovecha la estructura de la matriz de coeficientes cuando es dispersa, tal y como ocurre con los sistemas que surgen en problemas de análisis de estructuras o de elementos finitos.
	\end{nlist}
\end{nota}

\section{Análisis de error}
Ahora vamos a estudiar el error cometido al resolver mediante un método numérico un sistema de ecuaciones lineales unisolvente y con igual número de ecuaciones e incógnitas; el error relativo cometido al resolver de forma aproximada un sistema; errores derivados del método usado, o los debidos al redondeo, propagación...; y el error relativo controlado en función únicamente del vector de términos independientes y de la solución aproximada.

\begin{nprop}
Sea $A \in \mathbb{R}^{N \times N}$ una matriz regular y $x, u, b \in \mathbb{R}^N$ con $\Vert x \Vert \Vert b \Vert \neq 0$ y de forma que $Ax = b$. Entonces para cualquier norma en $\mathbb{R}^N$ se cumplen las desigualdades:
\[
\frac{1}{c(A)} \frac{\Vert Au-b \Vert } {\Vert b \Vert } 
\leq
\frac{\Vert x-u \Vert }{\Vert x \Vert }
\leq
c(A) \frac{\Vert Au-b \Vert } {\Vert b \Vert },
\]
donde c(A) es el condicionamiento de la matriz A relativo a la norma matricial inducida por $\Vert \cdot \Vert$.
\end{nprop}
	\begin{proof}
	Primera desigualdad:
	\[ \Vert Au - b \Vert \leq \Vert A \Vert \Vert x-u \Vert \]
	\[ \Vert x \Vert \leq \Vert A^{-1} \Vert \Vert b \Vert \]
	Entonces
	\[ \Vert Au-b \Vert \Vert x \Vert \leq c(A) \Vert x-u \Vert \Vert b \Vert \]
	Segunda desigualdad (razonamiento similar, tomando v=Au):
	\[
	\frac{\Vert x-u \Vert }{\Vert x \Vert } = \frac{\Vert A^{-1}b-A^{-1}v \Vert }{\Vert x \Vert }
	\leq
	\frac{\Vert A^{-1} \Vert \Vert b-v \Vert }{\Vert x \Vert }
	\leq
	\Vert A \Vert \Vert A^{-1} \Vert \frac{\Vert b-v \Vert }{\Vert b \Vert }
	=
	c(A) \frac{\Vert Au-b \Vert }{\Vert b \Vert }
	\]
	\end{proof}
	
La interpretación de estas desigualdades es que x es la solución del sistema Ax = b y $u$ es la solución aproximada.

La estimación del error relativo de la solución en función del condicionamiento de A y el error relativo generado al tomar Au por b es Au - b, que recibe el nombre de $\textbf{residuo}$, que en general no es nulo.

\begin{nprop}
Sean $N \geq 1$, $A, B \in \mathbb{R}^{N \times N}$ con A regular, $b,c \in \mathbb{R}^N$ y supongamos que el método iterativo
\[ \left\{ \begin{array}{l}
x_0 \; dado \\
n \geq 1 \Rightarrow x_n = Bx_{n-1} + c
\end{array}
\right. \]
converge a la solución del sistema Ax = b, cualquiera sea $x_0 \in \mathbb{R}^N$. Si además $\Vert \cdot \Vert$ es una norma en $\mathbb{R}^N$ con norma matricial inducida en $\mathbb{R}^{N \times N}$ denotada de igual forma, tal que $\Vert B \Vert < 1$, entonces, para todo $n \geq 1$ se tiene:
	\begin{nlist}
	\item $\Vert x_n - x \Vert \leq \frac{\Vert B \Vert ^n}{1 - \Vert B \Vert} \Vert x_1 - x_0$
	\item $\Vert x_n - x \Vert \leq \Vert B \Vert \Vert x_{n-1} - x \Vert$
	\item $\Vert x_n - x \Vert \leq \frac{\Vert B \Vert}{1 - \Vert B \Vert} \Vert x_n - x_{n-1} \Vert$
	\end{nlist}
\end{nprop}

	\begin{proof}
	$\newline$
	$\textbf{¿(i)?}$\\
	Supongamos $n \geq 1$, entonces
	\[ \Vert x_{n+1} - x_n \Vert = \Vert Bx_n - Bx_{n-1} \Vert \leq \Vert B \Vert \Vert x_n - x_{n-1} \Vert \]
	inductivamente tenemos que
	\[ \Vert x_{n+1} - x_n \Vert \leq \Vert B \Vert ^n \Vert x_1 - x_0 \Vert \]
	Sea $m \geq n \geq 1$, entonces
	\[ \Vert x_n - x_m \Vert \leq \sum_{j=0}^{m-n-1} \Vert x_{j+n+1} - x_{j+n} \Vert \leq \sum_{j=0}^{m-n-1} \Vert B \Vert ^{j+n} \Vert x_1 - x_0 \Vert \leq \frac{\Vert B \Vert ^n}{1- \Vert B \Vert} \Vert x_1 - x_0 \Vert \]
	Por último, tomamos límite en $m \rightarrow \infty$.\\
	$\textbf{¿(ii)?}$\\
	Supongamos $n \geq 1$, entonces
	\[ \Vert x_n - x \Vert = \Vert Bx_{n-1} + c - Bx - c \Vert \leq \Vert B \Vert \Vert x_{n-1} - x \Vert\]
	$\textbf{¿(iii)?}$\\
	Supongamos $n \geq 1$ y usando la desigualdad triangular tenemos que
	\[ \Vert x_{n-1} - x \Vert = \Vert x_{n-1} - x + x_n - x_n \Vert \leq \Vert x_{n-1} - x_n \Vert + \Vert x_n - x \Vert \]
	Usando (ii)
	\[ \Vert x_n - x \Vert \leq \Vert B \Vert \Vert x_n - x_{n-1} \Vert + \Vert B \Vert \Vert x_n - x \Vert \]
	Hemos obtenido la estimación pedida (falta reorganizar).
	\end{proof}

\begin{nota}
$\newline$
	 \begin{nlist}
	 \item[•] Las acotaciones (i) y (ii) dan una estimación del error absoluto, aunque hay una diferencia importante entre ambas: la primera no necesita conocer la solución exacta x.
	 \item[•] La estimación del error absoluto (iii) no solo se obtiene sin necesidad de conocer x sino que además constituye un criterio de parada cuando se programa el método numérico y se alcanza una tolerancia dada.
	 \end{nlist}
\end{nota}

\part{Tema 3. Interpolación}
En este tema vamos a aprender a calcular el polinomio que pasa por unas coordenadas dadas.

\section{Interpolación polinómica: Lagrange y Newton. Error de interpolación}
Sean las coordenadas $(x_0,y_0), (x_1,y_1),...,(x_N,y_N) \in \mathbb{R}^2 : x_i \neq x_j \; \; con \; i,j = 0,1,...N$, entonces existe una única función polinómica de grado menor o igual que N $p : \mathbb{R} \longrightarrow \mathbb{R}$ tal que $p(x_i) = y_i$ con i = 0,1,...N.\\
El problema que tenemos que resolver en este tema es determinar explícitamente el polinomio de interpolación $p \in \mathbb{P} _N$.\\
Para calcular p debemos calcular una base de $\mathbb{P} _N$ y sus propiedades. Veamos dos formas diferentes de hacerlo.

\subsection{Polinomio de interpolación tipo Lagrange}
\begin{nth}
Sean las coordenadas $(x_0,y_0), (x_1,y_1),...,(x_N,y_N) \in \mathbb{R}^2$, entonces
\[ \exists ! p \in \mathbb{P} _N : p(x_i) = y_j : x_i \neq x_j \; \; con \; i,j = 0,1,...N \; \; \Rightarrow p(x) = \sum_{i=0}^N y_il_i(x) \; \; con \; \; l_i(x) = \prod_{j=0 \wedge j \neq i}^N \frac{x-x_j}{x_i-x_j} \]
donde $\lbrace l_0(x), l_1(x),...,l_N(x) \rbrace$ es la base, dependiente de los $x_i$'s. Primero calculamos la base y luego el polinomio p(x).
\end{nth}

La fórmula anterior se conoce como $\textbf{forma de Lagrange del polinomio de
interpolación}$ y las funciones base $\textbf{polinomios de Lagrange o característicos}$.

\begin{ejemplo}
Sea $f: \mathbb{R} \longrightarrow \mathbb{R}$, $f(x) = e^x ( \forall x \in \mathbb{R} )$. Vamos a dar tres valores a la función para calcular el polinomio de Lagrange. Por ejemplo, en los puntos de abscisas -1, 0, 1:
\[ (x_0, y_0) = (-1,e^{-1}), \; (x_1,y_1) = (0,1), \; (x_2,y_2) = (1,e) \]
Ahora calculamos la base
\[ l_0(x) = \prod_{j=0 \wedge j \neq 0}^2 \frac{x-x_j}{x_0-x_j} = \frac{x^2}{2} - \frac{x}{2} \]
\[ l_1(x) = 1 - x^2 \]
\[ l_2(x) = \frac{x^2}{2} + \frac{x}{2} \]
Por último, calculamos el polinomio  de interpolación p(x):
\[ p(x) = \sum _{i=0}^2 e^{x_i}l_i(x) = \left( \frac{e}{2} + \frac{1}{2e} - 1 \right) x^2 + \left( \frac{e}{2} - \frac{1}{2e} \right) x + 1 \]
Dibujándolo, en trazo discontinuo es p(x) y en trazo continuo es f(x), observamos que pasa por las coordenadas dadas.

\[ \includegraphics[scale=0.3]{media/interplagrange.png} \]
\end{ejemplo}

\subsection{Forma de Newton del polinomio de interpolación}
\begin{nth}
Sean las coordenadas $(x_0,y_0), (x_1,y_1),...,(x_N,y_N) \in \mathbb{R}^2$, entonces
\[ \exists ! p \in \mathbb{P} _N : p(x_i) = y_j : x_i \neq x_j \; \; con \; i,j = 0,1,...N \; \; \Rightarrow p(x) = \sum_{i=0}^N f \left[ x_0,...,x_i \right] \omega _i(x) \]
con
\[ f \left[ x_0,...,x_i \right] = \frac{f \left[ x_1,...,x_i \right] - f \left[ x_0,...,x_{i-1} \right] }{x_i - x_0} \; \; con \; 1 \leq i \leq N\] 
y
\[ \omega _i(x) = \prod _{j=0}^{i-1} (x-x_j) \]
\end{nth}
La expresión anterior es la conocida como $\textbf{forma de Newton del polinomio de
interpolación}$ y las funciones base $\textbf{polinomios nodales}$.

Otra forma de calcular $f \left[ x_0,...,x_N \right]$ sería viéndolo como una "matriz", en la que solo nos fijaremos en la diagonal:

\[
\left. \begin{array}{c}
x_0 \\
x_1 \\
x_2 \\
\vdots \\
x_N
\end{array} \right| 
\begin{array}{ccccc}
f \left[ x_0 \right] & & & & \\
f \left[ x_1 \right] & f \left[ x_0,x_1 \right] & & & \\
f \left[ x_2 \right] & f \left[ x_1;x_2 \right] & f \left[ x_0,x_1,x_2 \right] & & \\
\vdots & \vdots & \vdots & \ddots & \\
f \left[ x_N \right] & f \left[ x_{N-1},x_n \right] & f \left[ x_{N-2},x_{N-1},x_n \right] & \cdots & f \left[ x_0,...,x_N \right] 
\end{array}
\]
donde $f \left[ x_0 \right] = f(x_0)$ y $\omega _0(x) = 1$.

\begin{ejemplo}
Dados los datos $(x_0,y_0)=(0.5,1), \; (x_1,y_1)=(1,0.2), \; (x_2,y_2)=(-0.25,1), \; (x_3,y_3)=(-0.5,0.2), \; (x_4,y_4)=(0.2,1/3)$, vamos a calcular las diferencias divididas.
\[
\left. \begin{array}{c}
0.5 \\
1 \\
-0.25 \\
-0.5 \\
0.2
\end{array} \right| 
\begin{array}{ccccc}
1 & & & & \\
0.2 & -1.6 & & & \\
1 & -0.64 & -1.28 & & \\
0.2 & 3.2 & -2.56 & 1.28 & \\
\frac{1}{3} & \frac{4}{21} & \frac{-1264}{189} & \frac{4876}{945} & \frac{-36664}{2835} 
\end{array}
\]
Algunas cuentas que hemos hecho para calcularlo son:
\[ f \left[ x_0,x_1 \right] = \frac{f \left[ x_1 \right] - f \left[ x_0 \right]}{x_1-x_0} = \frac{0.2-1}{1-0.5} = -1.6 \]
\[ f \left[ x_0,x_1,x_2 \right] = \frac{f \left[ x_1,x_2 \right] - f \left[ x_0,x_1 \right]}{x_2-x_0} = \frac{-0.64-(-1.6)}{-0.25-0.5} = -1.28 \]
\end{ejemplo}

\subsection{Error de interpolación. Convergencia y estabilidad. Polinomios de Chebyshev}
Primero, vamos a establecer la notación de algunos conceptos.

Sean $x_0,x_1,...,x_N \in \left[ a,b \right]$, una función $f \in$ C([a,b]) y su polinomio de interpolación $I_N^f \in \mathbb{P} _N$ tal que $I_N$ : C([a,b]) $\longrightarrow \mathbb{P} _N$ bien definida (unicidad polinomio interpolación). Además, tiene la propiedad de proyección, ya que $I_N^2 = I_N$.

\begin{ndef}
Definimos al $\textbf{error de interpolación} E_N^f(x)$ como
\[ E_N^f(x) := f(x) - I_N^f(x) \]
\end{ndef}

\begin{ndef}
\[ \Vert I_N \Vert _\infty := sup \; \left\lbrace \Vert I_N^f \Vert _\infty : f \in C( \left[ a,b \right] ), \Vert f \Vert _\infty = 1 \right\rbrace \]
\end{ndef}

\begin{nprop}
\[ \Vert I_N \Vert _\infty = up \; \left\lbrace \frac{\Vert I_N^f \Vert _\infty}{\Vert f \Vert _\infty} : f \in C( \left[ a,b \right] ), f \neq 0 \right\rbrace \]
\end{nprop}

\begin{ndef}[Función de Lebesgue]
Definimos la $\textbf{función de Lebesgue}$ como
\[ \lambda _N(x) := \sum_{i=0}^N \vert l_i(x) \vert \]
siendo {$l_0,...,l_N$} la base de los polinomios de Lagrange.
\end{ndef}

\begin{ndef}[Constante de Lebesgue]
Definimos la $\textbf{constante de Lebesgue}$ como
\[ \Lambda _N := \Vert \lambda _N \Vert _\infty \]
\end{ndef}

\begin{nprop}
\[ \Vert I_N \Vert _\infty = \Lambda _N \]
\end{nprop}

\begin{ncor}
Sea f $\in$ C([a,b]), entonces
\[ \Vert E_N^f \Vert _\infty \leq (1+\Lambda _N) \; inf \; \left\lbrace \Vert f-p \Vert _\infty : p \in \mathbb{P} _N \right\rbrace \]
\end{ncor}

\begin{nth}[Teorema de aproximación uniforme de Weierstrass]
\[ \lim_{N\rightarrow \infty} inf \; \left\lbrace \Vert f-p \Vert _\infty : p \in \mathbb{P} _N \right\rbrace = 0 \]
\end{nth}

Como $\lim _{N\rightarrow \infty} \Lambda _N = \infty$, no podemos asegurar cnvergencia uniforme, es decir, no podemos asegurar que $\lim_{N\rightarrow \infty} \Vert E_N^f \Vert _\infty = 0$.

Podemos ver si es estable segun $\Lambda _N$, que solo depende de los nodos y nos sirve para medir el condicionamiento.

\begin{nprop}
Sean los nodos $x_0,x_1,...,x_N$, los datos $(x_0,f(x_0)), (x_1,f(x_1)),...,(x_N,f(x_N))$ y los datos perturbados $(x_0,g(x_0)),(x_1,g(x_1)),...,(x_N,g(x_N))$. Entonces
\[ \Vert I_N^f - I_N^g \Vert _\infty = max \; \left\lbrace \left\vert \sum_{i=0}^N (f(x_i)-g(x_i))l_i(x) \right\vert : x \in \left[ a,b \right] \right\rbrace \leq \Lambda _N max \; \left\lbrace \vert f(x_i)-g(x_i) : i=0,1,...,N \right\rbrace \]
\end{nprop}

\begin{ejemplo}
Sea $f : \left[ -1,1 \right] \longrightarrow \mathbb{R}$, $f(x) := e^x$. Elegimos 21 nodos uniformemente distribuidos en [-1,1], por ejemplo
\[ i=0,1,...,20 \Rightarrow x_i := -1 + \frac{2i}{20} \]
Los datos que obtenemos son $(x_i,f(x_i))$ y los datos perturbados son $(x_i,g(x_i))$ con $g(x_i)=f(x_i+(-1)^i10^{-4})$.

Tenemos que $max \; \left\lbrace \vert f(x_i)-g(x_i) : i=0,...,20 \right\rbrace = 2.7184 \cdot 10^{-4}$

Pero $\Vert I_{20}f-I_{20}g \Vert _\infty = 1.1964$

Por lo que hay mal condicionamiento $\Lambda _{20} = 10986.7058$.
\[ \Vert I_{20}f-I_{20}g \Vert _\infty \leq \Lambda _{20} max \; \left\lbrace \vert f(x_i)-g(x_i) : i=0,...,20 \right\rbrace \]
\[ 1.1964 \leq 10986.7958 \cdot 0.2718 \cdot 10^{-3} = 2.9862 \]
\end{ejemplo}

\begin{nprop}
Sean $x_0,x_1,...,x_N$ números reales distintos, sea $x \in \mathbb{R}$ y sean $a:=min \; \left\lbrace x,x_0,x_1,...,x_N \right\rbrace$ y $b:=max \; \left\lbrace x,x_0,x_1,...,x_N \right\rbrace$. Supongamos además que $f \in C^{N+1}$([a,b]). Entonces existe $\xi \in$ ]a,b[ tal que
\[ E_N^{f(x)}=\frac{f^{N+1)}( \xi )}{(N+1)!} \omega _{N+1}(x), \]
donde $omega _{N+1}(x)$ es el polinomio nodal de grado N+1.
\end{nprop}

\[ f(x) = I_Nf(x) + \frac{f^{N+1)}( \xi )}{(N+1)!} \omega _{N+1}(x) \]

Es análoga a la fórmula de Taylor:

\begin{ncor}
Bajo las condiciones de la proposición anterior
\[ \Vert E_N^f \Vert _\infty \leq \frac{\Vert f^{N+1)} \Vert _\infty }{(N+1)!} (b-a)^{N+1} \]
\end{ncor}

La convergencia es uniforme: $f \in C^{\infty}$([a,b])
\[ \lim_{N \rightarrow \infty} \frac{\Vert f^{N+1)} \Vert _\infty }{(N+1)!} (b-a)^{N+1} = 0 \; \Rightarrow \; \lim_{N \rightarrow \infty} \Vert E_N^f \Vert _\infty = 0 \]

La elección de los nodos influye en el error de interpolación. En general, aun siendo mejorable, para nodos equidistantes tenemos que
\[ \Vert E_N^f \Vert _\infty \leq \frac{\Vert f^{N+1)} \Vert _\infty}{(N+1)!}h^{N+1} \]
error de interpolación de orden $O \left( h^{N+1} \right)$.

[-1,1], [a,b] $\leftrightsquigarrow$ [-1,1] isomorfismo afín. Sea $\theta \in \mathbb{R}$, $N \geq 1$
\[ cos(N+1) \theta + cos(N-1) \theta = 2cos \theta cos N\theta \]
\[\Downarrow \]
\[ cos(N+1) \theta = 2cos \theta cos N\theta - cos(N-1) \theta \]
\[ \Downarrow \]
\[ \exists T_N \in \mathbb{P}_N : T_N (cos \theta ) = cos N\theta \]
\[ cos : \left[ 0,\pi \right] \longleftrightarrow \left[ -1.1 \right] biyección \]
\[ x = cos \theta \]
\[ cos (N+1) \theta = 2 cos \theta cos N\theta - cos(N-1) \theta \]
y
\[ T_N(cos \theta ) = cos N\theta \]

\[
\left| 
\begin{array}{c}
T_0(x)=1 \\
T_1(x)=x \\
i=0,1,2,... \Rightarrow T_{i+2}(x)=2xT_{i+1}(x)-T_i(x) \\
\end{array}
\right.
\]
donde $T_i$ es el $\textbf{polinomio de Chebyshev}$ de grado i. Sus propiedades importantes son:

\begin{nlist}
\item[•] $T_N \in \mathbb{P}_N$ con N ceros reales, todos en [-1,1] y coeficiente líder $2^{N-1}$.
\item[•] $T_N \in C$([-1,1]), $\Vert T_N \Vert _\infty$ = 1.
\end{nlist}

Los $\textbf{nodos de Chebyshev}$ son:
\[
\left| 
\begin{array}{c}
T_N \in \mathbb{P}_N \\
T_N(cos \theta ) = cos(N \theta ) \\
cos(N\theta ) = 0 \\
\end{array}
\right.
x_i^{(N)} = cos \frac{2i+1}{2N}\pi \; , \; \; (i=0,...,N-1) los N ceros de T_N
\]

\begin{nth}
Sea $N \geq 1$ y sea $p \in \mathbb{P} _N$ un polinomio con coeficiente líder 1. Entonces
\[ max \; \left\lbrace \vert p(x) \vert : x \in \left[ -1,1 \right] \right\rbrace \geq \frac{1}{2^{N-1}} \]
\end{nth}

Con los nodos de Chebyshev tenemos que
\[ \Vert E_N^f \Vert _\infty \leq \frac{\Vert f^{N+1)} \Vert _\infty}{(N+1)!} \frac{1}{2^N} \]

\subsection{Otros problemas de interpolación: Hermite y caso general}
Sean los nodos distintos $x_0,x_1,...,x_N \in$ [a,b], los órdenes de derivación $m_0,m_1,...,m_N \geq 0$ y la función $f \in C^M$([a,b]), $M:= max \left\lbrace m_i : i=0,...,N \right\rbrace$. El $\textbf{problema de interpolación de Hermite}$ consiste en encontrar $p \in \mathbb{P}$ de grado mínimo K con
\[ i=0,1,...,N \Rightarrow \left[ j=0,1,...,m_i \Rightarrow p^{j)}(x_i) = f^{j)}(x_i) \right] \]

\[ p \in \mathbb{P} _K : p^{j)}(x_i) = f^{j)}(x_i) \Leftrightarrow L(p) = f^{j)}(x_i) \]
con $L : \mathbb{P} _K \longrightarrow \mathbb{R}$ forma lineal
\[ p \mapsto L(p) := p^{j)}(x_i) \]

El $\textbf{problema general de interpolación}$ consiste en, sea E espacio vectorial real de dimensión N, $L_1,...,L_N : E \longrightarrow \mathbb{R}$ formas lineales, $d_1,...,d_N \in \mathbb{R}$, encontrar un único $p \in E : \left[  i=1,...,N \Rightarrow L_i(p) = d_i \right] $

\begin{nprop}
Sea E un espacio vectorial (real) de dimensión $N \geq 1$, sean $L_1,...,L_N : E \longrightarrow \mathbb{R}$ formas lineales y sea {$p_1,...,p_N$} una base de E. Entonces, el problema general de interpolación admite una única solución, cualesquiera sean $d_1,...,d_N \in \mathbb{R}$, si, y solo si,
\[ det \left[ L_i(p_j) \right] _{i,j=1}^N \neq 0 \]
\end{nprop}

$\left[ L_i(p_j) \right] _{i,j=1}^N$ es la matriz de coeficientes de un sistema cuadrado.

Unisolvencia para cualesquiera $d_1,...,d_N \Leftrightarrow$ unisolvencia para $d_1 = \cdots = d_N = 0$.

A partir de ahora, vamos a enfocar el problema con Lagrange.

Sea E espacio vectorial real de dimensión N, $L_1,...,L_N : E \longrightarrow \mathbb{R}$ formas lineales, $d_1,...,d_N \in \mathbb{R}$, el problema general de interpolación consiste ne encontrar un único $p \in E : \left[  i=1,...,N \Rightarrow L_i(p) = d_i \right]$

Sea {$l_1,...,l_N$} base de E de Lagrange
\[ i,j = 1,...,N \Rightarrow L_i(l_j) = \delta _{ij} \Rightarrow p:= \sum_{i=1}^N d_il_i \]
que es la solución del problema general de interpolación. La dificultad está en determinar la base de Lagrange, que siempre existe. Veamos los distintos casos que existen.

\begin{nlist}
\item $(x_0,y_0,d_0),(x_1,y_1,d_1),...,(x_N,y_N,d_N) \in \mathbb{R}^3 : i,j=0,1,...,N \Rightarrow x_i \neq x_j $
\[ E := \mathbb{P}_{2N+1}, \; \; \left\lbrace 1,x,...,x^{2N+1} \right\rbrace \; base \]
\[ i = 0,1,...,N \Rightarrow L_{2i}(p) := p(x_i), \; \; L_{2i+1}(p) = p'(x_i) \]
\[ encontrar \; un \; unico \; p \in E : \left[ i = 0,1,...,N \Rightarrow p(x_i)=y_i, \; p'(x_i) = d_i \right] \]
\[ det \left[ L_i(p_j) \right] _{i,j=0}^{2N+1} \neq 0 \]
\[ p(x_0) = p'(x_0) = 0 \Rightarrow p(x)=(x-x_0)^2q_0(x), \; para \; cierto \; q_0 \in \mathbb{P}_{2N-1} \]

\[ \exists \alpha \in \mathbb{R}: p(x) = \alpha \prod_{i=0}^{N+1}(x-x_i)^2 \]
Como $p \in \mathbb{P}_{2N+1}$, entonces p=0. Hay unisolvencia.

La base de Lagrange nos lleva a los polinomios de Hermite {$h_0(x),h_1(x),...,h_{2N+1}(x)$}.
Sea i=0,1,...,N, entonces
\[ h_{2i}(x)=(1-2(x-x_i)l'_i(x_i))l_i^2(x) \]
\[ h_{2i+1}(x) = (x-x_i)l_i(x)^2 \]
{$l_0(x),l_1(x),...,l_N(x)$} polinomios de Lagrange
\[ i=0,1,...,N \Rightarrow l_i(x) = \prod_{j=0 \bigwedge j\neq i}^N \frac{x-x_j}{x_i-x_j}\]
La solución es
\[p(x) = \sum_{i=0}^N (y_ih_{2i}(x) + d_id_{2i+1}(x)) \]

	\begin{ejemplo}
	Sean los nodos $x_0=-1, x_1=-0.5, x_2=0, x_3=0.5, x_4=1$, sean los valores $y_0=1, y_1=2, y_2=0, y_3=-2, y_4=-1$ y sean las derivadas $d_0=0, d_1=1, d_2=-1, d_3=1, d_4=0$. Entonces
	\[ p(x) = -x - \frac{833}{18}x^3 + \frac{385}{2}x^5 - \frac{740}{3}x^7 + \frac{904}{9}x^9 \]
	\end{ejemplo}

\item $(x_0,d_0,d_1,...,d_N) \in \mathbb{R}^{N+2}$
\[ E:= \mathbb{P}_N, \; \left\lbrace 1,x,...,x^N \right\rbrace \; base \]
\[ i=0,1,...,N \Rightarrow L_i(p) := p^{i)}(x_0) \]
El problema es encontrar un único $p \in E : \left[ i=0,1,...,N \Rightarrow p^{i)}(x_0) = d_i \right] $
\[ det \left[ L_i(p_j) \right] _{i,j=0}^N = det \left[
\begin{array}{ccccc}
1 & x_0 & x_0^2 & \cdots & x_0^N \\
0 & 1 & 2x_0 & \cdots & Nx_0^{N-1} \\
0 & 0 & 2 & \cdots & N(N-1)x_0^{N-2} \\
\vdots & \vdots & \vdots & & \vdots \\
0 & 0 & 0 & \cdots & N!
\end{array}
\right] \neq 0 \]
Hay unisolvencia.

Con la base de Lagrange obtenemos los polinomios de Taylor {$t_0(x),t_1(x),...,t_N(x)$}
\[i=0,1,...,N \Rightarrow t_i(x) = \frac{(x-x_0)^i}{i!} \]
Solución:
\[ p(x) = \sum_{i=0}^N \frac{d_i}{i!}(x-x_0)^i \]
$d_i \leftrightsquigarrow f^{i)}(x_0)$ polinomio de Taylor.

\end{nlist}

\section{Interpolación mediante funciones splines}
Para mejorar la precisión tenemos que hacer una partición del intervalo [a,b] y que el grado polinomial sea bajo. Sea P una $\textbf{partición}$ de [a,b] tal que P={$a=x_0 < x_1 < \cdots < x_N = b$}.

\begin{ndef}[Espacio de funciones splines]
Dados un intervalo [a,b] y una partición P del mismo, el $\textbf{espacio de funciones splines}$ de clase k y grado m viene dado por
\[ \mathbb{S}_m^k(P) := \left\lbrace s \in C^k ( \left[ a,b \right] ) : i=0,1,...,N-1 \Rightarrow s_{\vert \left[ x_i, x_{i+1} \right]} \in \mathbb{P} _m \right\rbrace \]
\end{ndef}

Tenemos que $\mathbb{S}_m^m = \mathbb{P}_m \; y \; k<m$.

Empecemos en el espacio vectorial $\mathbb{S}_1^0(P)$. Tenemos que dim$\mathbb{S}_1^0(P)$=2N-(N-1)=N+1 y la base usual es {$B_0(x),B_1(x),...,B_N(x)$}.

La siguiente definición es muy importante.

\begin{ndef}[Base usual]
Sean a<b y sea P={$a=x_0<x_1<\cdots < x_N=b$} una partición del intervalo [a,b]. La $\textbf{base usual}$ del espacio $\mathbb{S}_1^0(P)$ viene dada por
\[ i=0,1,...,N \Rightarrow B_i(x) := \left\lbrace
\begin{array}{ll}
\frac{x-x_{i-1}}{x_i-x_{i-1}}, & si \; x \in \left[ x_{i-1},x_i \right] \\
\frac{x_{i+1}-x}{x_{i+1}-x_i}, & si \; x \in \left[ x_i,x_{i+1} \right] \\
0, & fuera \\
\end{array}
\right.
\]
\end{ndef}

Dados un intervalo [a,b], una partición P={$a=x_0<x_1<\cdots < x_N=b$} y una función $f:$ [a,b] $\longrightarrow \mathbb{R}$, se considera el $\textbf{problema de interpolación en}$ $\mathbb{S}_1^0(P)$

\[encontrar \; s \in \mathbb{S}_1^0(P) : \left[ i=0,1,...,N \Rightarrow s(x_i) = f(x_i) \right] \]

La estructura del problema de interpolación general es claramente unisolvente. Con la base de Lagrange tenemos {$B_0(x), B_1(x),...,B_N(x)$}. La solución, notando s=$S_N^1f$
\[ S_N^1f(x) = \sum_{i=0}^N f(x_i)B_i(x) \]

\begin{ndef}[Error de interpolación]
\[ E_N^{f(x)} = f(x) - S_N^1f(x) \]
\end{ndef}

Sea $f \in C^2$([a,b])
\[ i=0,1,...,N-1, \; x \in \left[ x_i,x_{i+1} \right] \Rightarrow \vert f(x) - S_N^1f(x) \vert \leq \frac{\Vert f'' \Vert _\infty}{8}(x_{i+1}-x_i)^2 \]
\[ \Downarrow \]
\[ \Vert f-S_N^1f \Vert _\infty \leq \frac{\Vert f'' \Vert _\infty}{8} \left( max \; \left\lbrace (x_{i+1}-x_i):i=0,...,N-1 \right\rbrace \right) ^2 \]

Hemos probado:

\begin{nprop}
Con la notación anterior, si $f \in C^2$([a,b]), entonces
\[ \lim_{N\rightarrow \infty} max \; \left\lbrace (x_{i+1}-x_i):i=0,...,N-1 \right\rbrace = 0 \qquad \Rightarrow \qquad \lim_{N \rightarrow \infty} \Vert f - S_N^1f \Vert _\infty = 0 \]
\end{nprop}
\subsection{Funciones splines lineales}

\subsection{Funciones splines cúbicas}